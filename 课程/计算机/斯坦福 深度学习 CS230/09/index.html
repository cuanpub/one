



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="美剧注解笔记，串串英语提供基于分级阅读(leveled reading)和可理解输入(comprehensible input)理论的,包括播放器/阅读器/在线内容等,一系列产品,内容及解决方案">
      
      
        <link rel="canonical" href="https://english.thingswell.cn/%E8%AF%BE%E7%A8%8B/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%96%AF%E5%9D%A6%E7%A6%8F%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20CS230/09/">
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-4.1.0">
    
    
      
        <title>斯坦福 深度学习 CS230 - 09</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/application.982221ab.css">
      
        <link rel="stylesheet" href="../../../../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../../../../assets/javascripts/modernizr.01ccdecf.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../../assets/fonts/material-icons.css">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-148029185-1", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    

    <script data-ad-client="ca-pub-3535900728641832" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#_1" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://english.thingswell.cn/" title="串串英语" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              串串英语
            </span>
            <span class="md-header-nav__topic">
              09
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://english.thingswell.cn/" title="串串英语" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    串串英语
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="https://english.thingswell.cn/" title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="https://english.thingswell.cn/help/about/" title="说明" class="md-nav__link">
      说明
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="https://english.thingswell.cn/help/content/" title="内容" class="md-nav__link">
      内容
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="https://english.thingswell.cn/help/contact/" title="联系" class="md-nav__link">
      联系
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="https://english.thingswell.cn/zan/" title="充电" class="md-nav__link">
      充电
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" title="文本" class="md-nav__link">
    文本
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" title="知识点" class="md-nav__link">
    知识点
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>09</h1>
                
                <p><a href="../"><img src="/img/back.png" style="width:32px;height:32px;"></a><br><br></p>
<!-- begin -->

<!-- content -->

<p><br></p>
<h4 id="_1">文本</h4>
<p><font color="#FF0000" style="background-color: #FF0000"/>████</font>&nbsp;&nbsp;&nbsp;&nbsp;重点词汇<br>
<font color="#800000" style="background-color: #800000"/>████</font>&nbsp;&nbsp;&nbsp;&nbsp;难点词汇<br>
<font color="#0000FF" style="background-color: #0000FF"/>████</font>&nbsp;&nbsp;&nbsp;&nbsp;生僻词<br>
<font color="#00BFFF" style="background-color: #00BFFF"/>████</font>&nbsp;&nbsp;&nbsp;&nbsp;词组 &amp; 惯用语<br>
<br>[学习本文需要基础词汇量：<x-rating>5</x-rating>,000 ]<br>[本次分析采用基础词汇量：<x-rating2>6</x-rating2>,000 ]<br><p style="line-height: 150%">Hi everyone and welcome [NOISE] to Lecture 9 uh, for CS230.<br><br>Uh, today we are going to discuss an advanced topic uh,<br><br>that will be kind of the,<br><br>the marriage between deep learning and<br><br>another field of AI which is <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=reinforcement"><span style="border-bottom:1px solid #FF0000;">reinforcement</span></a></object> learning,<br><br>and we will see a practical uh,<br><br>application in how deep learning methods can be <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=plug in"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">plugged in</span></a></object> another family of <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=algorithm"><span style="border-bottom:1px solid #FF0000;">algorithm</span></a></object>.<br><br>So, it's interesting because deep learning methods and deep <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=neural network"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=neural"><span style="border-bottom:1px solid #FF0000;">neural</span></a></object> networks</span></a></object><br><br>have been shown to be very good uh, function <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=approximators"><span style="border-bottom:1px solid #0000FF;">approximators</span></a></object>.<br><br>Essentially, that's what they are.<br><br>We're giving them data so that they can <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=approximate"><span style="border-bottom:1px solid #FF0000;">approximate</span></a></object> a function.<br><br>There are a lot of different fields which require these function <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=approximators"><span style="border-bottom:1px solid #0000FF;">approximators</span></a></object>,<br><br>and deep learning methods can be <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=plug in"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">plugged in</span></a></object> all these methods.<br><br>This is one of these examples.<br><br>So, we'll first uh, motivate uh,<br><br>the, the setting of <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=reinforcement"><span style="border-bottom:1px solid #FF0000;">reinforcement</span></a></object> learning.<br><br>Why do we need <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=reinforcement"><span style="border-bottom:1px solid #FF0000;">reinforcement</span></a></object> learning?<br><br>Why cannot, what <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=wh"><span style="border-bottom:1px solid #FF0000;">wh</span></a></object> - why can't we use deep learning methods to solve everything?<br><br>There is some set of methods that we cannot solve with<br><br>deep learning and <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=reinforcement"><span style="border-bottom:1px solid #FF0000;">reinforcement</span></a></object> learning <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=meth"><span style="border-bottom:1px solid #FF0000;">meth</span></a></object> - uh,<br><br>re-reinforcement learning applications are examples of that.<br><br>Uh, we will see an example uh,<br><br>to introduce <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=an algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">an <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=algorithm"><span style="border-bottom:1px solid #FF0000;">algorithm</span></a></object></span></a></object>, a re - <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=reinforcement"><span style="border-bottom:1px solid #FF0000;">reinforcement</span></a></object> learning <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=algorithm"><span style="border-bottom:1px solid #FF0000;">algorithm</span></a></object> called Q-Learning,<br><br>and we will add deep learning to this <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=algorithm"><span style="border-bottom:1px solid #FF0000;">algorithm</span></a></object> and make it Deep Q-Learning.<br><br>[NOISE] Uh, as we've seen with uh,<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=generative"><span style="border-bottom:1px solid #800000;">generative</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=adversarial"><span style="border-bottom:1px solid #FF0000;">adversarial</span></a></object> networks and also deep <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=neural network"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=neural"><span style="border-bottom:1px solid #FF0000;">neural</span></a></object> networks</span></a></object>,<br><br>most models are hard to train.<br><br>We've had, we had to come up with <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=xavier"><span style="border-bottom:1px solid #800000;">Xavier</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=initialization"><span style="border-bottom:1px solid #800000;">initialization</span></a></object>, with <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=dropout"><span style="border-bottom:1px solid #FF0000;">dropout</span></a></object>,<br><br>with batch norm, and <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=myriads"><span style="border-bottom:1px solid #FF0000;">myriads</span></a></object>,<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=myriads"><span style="border-bottom:1px solid #FF0000;">myriads</span></a></object> of methods to make these deep <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=neural network"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=neural"><span style="border-bottom:1px solid #FF0000;">neural</span></a></object> networks</span></a></object> trained.<br><br>In GANs, we had to use methods as well in order to train GANs,<br><br>and tricks and <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=hacks"><span style="border-bottom:1px solid #FF0000;">hacks</span></a></object>.<br><br>So, here we will see some of the tips and tricks to train Deep Q-Learning,<br><br>which is a reinforcement learning <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=algorithm"><span style="border-bottom:1px solid #FF0000;">algorithm</span></a></object>.<br><br>[NOISE] And at the end,<br><br>we will have a<br><br>guest speaker coming uh,<br><br>to talk about advanced topics which are mostly research,<br><br>which combine deep learning and reinforcement learning.<br><br>Sounds good? Okay. Let's go.<br><br>[NOISE] So, deep reinforcement learning is a very recent field I would say.<br><br>Although both fields are,<br><br>are - reinforcement learning have,<br><br>has existed for a long time.<br><br>Uh, only recently, it's been shown that using deep learning as a way to<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=approximate"><span style="border-bottom:1px solid #FF0000;">approximate</span></a></object> the functions that play<br><br>a big role in reinforcement learning <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=algorithms"><span style="border-bottom:1px solid #FF0000;">algorithms</span></a></object> has worked a lot.<br><br>So, one example is AlphaGo and uh,<br><br>you probably all have heard of it.<br><br>It's <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=google"><span style="border-bottom:1px solid #0000FF;">Google</span></a></object> DeepMind's<br><br>AlphaGo has uh, beaten world champions in a game called the game of Go,<br><br>which is a [NOISE] very,<br><br>a very old strategy old game,<br><br>and the one on the right here um,<br><br>or on, on your right,<br><br>human level control through deep reinforcement learning is also uh<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=deepmind"><span style="border-bottom:1px solid #0000FF;">DeepMind</span></a></object>, Google's <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=deepmind"><span style="border-bottom:1px solid #0000FF;">DeepMind</span></a></object> paper that came<br><br>out and hit the headlines on the front page of Nature,<br><br>which is uh, one of the leading uh,<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=multidisciplinary"><span style="border-bottom:1px solid #FF0000;">multidisciplinary</span></a></object> peer review journals uh, in the world.<br><br>And they've shown that with deep learning <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=plug in"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">plugged in</span></a></object> a reinforcement learning setting,<br><br>they can train an agent that beats human level in a variety of games and in fact,<br><br>these are <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=atari"><span style="border-bottom:1px solid #800000;">Atari</span></a></object> games.<br><br>So, they've shown actually that their algorithm,<br><br>the same algorithm reproduced for a large number of games,<br><br>can beat humans on all of these games. Most of these games.<br><br>Not all of these games. So, these are two examples,<br><br>although they use different <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=sub"><span style="border-bottom:1px solid #FF0000;">sub</span></a></object> techniques of uh, reinforcement learning.<br><br>they both include some deep learning aspect in it.<br><br>And today we will mostly talk about the human level controlled through<br><br>deep reinforcement learning or so-called deep Q network,<br><br>presented in this paper.<br><br>So, let's, let's start with,<br><br>with motivating reinforcement learning using the, the AlphaGo setting.<br><br>Um, this is a board of Go and the picture comes from DeepMind's <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=blog"><span style="border-bottom:1px solid #FF0000;">blog</span></a></object>.<br><br>Uh, so Go you can think of it as a strategy game,<br><br>where you have <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=a grid"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">a grid</span></a></object> that is up to 19 by 19 and you have two players.<br><br>One player has white stones and one player has black stones,<br><br>and at every step in the game,<br><br>you can position a stone on the, on the board.<br><br>On one of the grid cross.<br><br>The goal is to surround your opponent,<br><br>so to maximize your territory by surrounding your opponent.<br><br>And it's a very complex game for different reasons.<br><br>Uh, one reason is that you have to be you,<br><br>you cannot be <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=shortsighted"><span style="border-bottom:1px solid #800000;">shortsighted</span></a></object> in this game.<br><br>You have to have a long-term strategy.<br><br>Another reason is that the board is so big.<br><br>It's much bigger than a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=chessboard"><span style="border-bottom:1px solid #800000;">chessboard</span></a></object>, right?<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=chessboard"><span style="border-bottom:1px solid #800000;">Chessboard</span></a></object> is eight by eight.<br><br>So, let me ask you a question.<br><br>If you had to solve or build an agent that solves this game and beats<br><br>humans or plays very well at least with<br><br>deep learning methods that you've seen so far, how would you do that?<br><br>[NOISE]<br><br>Someone wants to try.<br><br>[NOISE] So, let's say you have a,<br><br>you have to collect the data set because in,<br><br>in classic supervised learning,<br><br>we need a data set with x and y.<br><br>What do you think would be your x and y?<br><br>[NOISE] Yeah.<br><br>The game board is the input, the output probability of victory in that position.<br><br>Okay. Input is game board, and output is probability of victory in that position.<br><br>So, that's, that's a good one I think. Input output.<br><br>What's the issue with that one?<br><br>[NOISE] So, yeah.<br><br>[<object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=inaudible"><span style="border-bottom:1px solid #FF0000;">inaudible</span></a></object>]<br><br>Yeah. It's super hard to represent what the probability of winning is from this board.<br><br>Even, like nobody can tell.<br><br>Even if I ask [NOISE] an experienced human to come and tell us what's<br><br>the probability of black winning in this or white winning [NOISE] in,<br><br>in this setting, they wouldn't be able to tell then [NOISE].<br><br>So, this is a little more complicated.<br><br>Any other ideas of data sets? Yeah.<br><br>[NOISE] [<object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=inaudible"><span style="border-bottom:1px solid #FF0000;">inaudible</span></a></object>].<br><br>Okay. <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=good point"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">Good point</span></a></object>. So, we could have the grid like this one and then this is the input,<br><br>and the output would be the move, the next action taken by probably a professional player.<br><br>So, we would just watch professional players playing and we would record their moves,<br><br>and we will build a data set of what is a professional move,<br><br>and we hope that our network using these input/outputs,<br><br>will at some point learn how the professional players play and given an input,<br><br>a state of the board,<br><br>would be able to decide of the next move.<br><br>What's the issue with that?<br><br>Yeah.<br><br>[<object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=inaudible"><span style="border-bottom:1px solid #FF0000;">inaudible</span></a></object>]<br><br>Yes. [NOISE] We need a whole lot of data.<br><br>Why? And - and you, you said it.<br><br>You - you said because, uh,<br><br>we need basically to represent all types of positions of the board, all states.<br><br>So, if, if you were actually.<br><br>Let's - let's do that.<br><br>If we were <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=to compute"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=compute"><span style="border-bottom:1px solid #FF0000;">compute</span></a></object></span></a></object> the number of possible states,<br><br>uh, of this board, what would it be?<br><br>[NOISE] It's a 19 by 19 board.<br><br>[NOISE].<br><br>Remember what we did with <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=adversarial"><span style="border-bottom:1px solid #FF0000;">adversarial</span></a></object> examples.<br><br>We did it for <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pixels"><span style="border-bottom:1px solid #FF0000;">pixels</span></a></object>, right?<br><br>[NOISE] Now, we're doing it for the board.<br><br>So, what's - the, the question first is. Yeah, You wanna try?<br><br>Uh, three to the 19 squared.<br><br>Yeah. Three to the power 19 [NOISE] times 19.<br><br>Or 19 squared. Yeah. So, why is it that?<br><br>[NOISE] So, why is it? Is it this?<br><br>Each spot can have [<object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=inaudible"><span style="border-bottom:1px solid #FF0000;">inaudible</span></a></object>] uh,<br><br>have no stone a white stone or a black.<br><br>Yeah. Each spot and there are 19 times 19 spots,<br><br>can have three state basically.<br><br>No stone, white stone, or black stone.<br><br>But this is the all possible state.<br><br>This is about 10 to the 170.<br><br>So, it's super, super big.<br><br>So, we can probably not get even close to that by observing professional players.<br><br>First because we don't have enough professional players and because,<br><br>uh, we are humans and we don't have <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=infinite life"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=infinite"><span style="border-bottom:1px solid #FF0000;">infinite</span></a></object> life</span></a></object>.<br><br>So, the professional players can not play forever.<br><br>They might get tired as well.<br><br>Uh, but, so one issue is the state space is too big.<br><br>Another one is that the ground truth probably would be wrong.<br><br>It's not because you're a professional player that you will play<br><br>the best move every time, right?<br><br>Every player has their own strategy.<br><br>So, the ground truth we're,<br><br>we're having here is not necessarily true,<br><br>and our network might, might not be able to beat these human players.<br><br>What we are looking into here is <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=an algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">an algorithm</span></a></object> that beats humans.<br><br>Okay. Second one, too many states in the game as you mentioned,<br><br>and third one we will likely not <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=generalize"><span style="border-bottom:1px solid #FF0000;">generalize</span></a></object>.<br><br>The reason we will not <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=generalize"><span style="border-bottom:1px solid #FF0000;">generalize</span></a></object> is because in classic supervised learning,<br><br>we're looking for patterns.<br><br>If I ask you to build <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=an algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">an algorithm</span></a></object> to detect cats versus dogs,<br><br>it will look for what the pattern of a cat is versus what the pattern of the dog is in,<br><br>and in the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=convolutional"><span style="border-bottom:1px solid #0000FF;">convolutional</span></a></object> filters, we will learn that.<br><br>In this case, it's about a strategy.<br><br>It's not about a pattern.<br><br>So, you have to understand the process<br><br>of winning this game in order to make the next move.<br><br>You cannot <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=generalize"><span style="border-bottom:1px solid #FF0000;">generalize</span></a></object> if you don't understand this process of long-term strategy.<br><br>So, we have to incorporate that,<br><br>and that's where RL comes into place.<br><br>RL is reinforcement learning,<br><br>a method that would - could be described with one sentence<br><br>as automatically learning to make good sequences of decisions.<br><br>So, it's about the long-term.<br><br>It's not about the short-term,<br><br>and we would use it generally when we have delayed labels,<br><br>like in this game.<br><br>The label that you mentioned at the beginning was probability of victory.<br><br>This is a long-term label.<br><br>We cannot get this label now but over time,<br><br>the closer we get to the end,<br><br>the better we are at seeing the victory or not,<br><br>and it's for to make sequences of decisions.<br><br>So, we make a move then the opponent makes a move.<br><br>Then we make another move, and<br><br>all the decisions of these moves are <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=correlate with"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">correlated with</span></a></object> each other.<br><br>Like you have to plan in, in advance.<br><br>When you are human you do that,<br><br>when you <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=play chess"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">play <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=chess"><span style="border-bottom:1px solid #FF0000;">chess</span></a></object></span></a></object>, when you play Go [NOISE].<br><br>So examples of RL applications can be <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=robotics"><span style="border-bottom:1px solid #FF0000;">robotics</span></a></object><br><br>and it's still a research topic how deep RL can change <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=robotics"><span style="border-bottom:1px solid #FF0000;">robotics</span></a></object>,<br><br>but think about having <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=a robot"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">a robot</span></a></object> walking from here and you wanna send it there.<br><br>You'll want to send the robot there.<br><br>What you're teaching the robot is if you get there it's good, right? It's good.<br><br>You achieve the task, but I<br><br>cannot give you the probability of getting there at every point.<br><br>I can help you out by giving you<br><br>a reward when you arrive there and let you trial and error.<br><br>So, the robot will try and <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=randomly"><span style="border-bottom:1px solid #FF0000;">randomly</span></a></object><br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=initialized"><span style="border-bottom:1px solid #800000;">initialized</span></a></object>, the robot would just fall down at the first, at first.<br><br>Gets a negative reward.<br><br>Then, repeats.<br><br>This time the robot knows that it shouldn't fall down.<br><br>It shouldn't go down.<br><br>You should probably go this way.<br><br>So, through trial and error and reward on the long-term,<br><br>the - the robot is supposed to learn this pattern.<br><br>Another one is games and that's the one we will see today.<br><br>Uh, games can be represented as, as,<br><br>as a set of reward for reinforcement learning algorithm.<br><br>So, this is where you win,<br><br>this is where you lose.<br><br>Let <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the algorithm</span></a></object> play and figure out what<br><br>winning means and what losing means, until it learns.<br><br>Okay. The problem with using deep learning is that <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the algorithm</span></a></object><br><br>will not learn cause this reward is too long-term.<br><br>So, we're using reinforcement learning, and finally advertisements.<br><br>So, a lot of advertisements,<br><br>um, are real-time bidding.<br><br>So, you wanna know given a budget when you wanna invest this budget,<br><br>and this is a long-term strategy planning as well,<br><br>that reinforcement learning can help with.<br><br>Okay. [NOISE] So, this was the motivation of reinforcement learning.<br><br>We're going to jump to a concrete example that is<br><br>a super vanilla example to understand Q-Learning.<br><br>So, let's start with this game or environment.<br><br>So, we call that an environment generally and it has several states.<br><br>In this case five states.<br><br>So, we have these states and we can define rewards, which are the following.<br><br>So, let's see what is our goal in this game.<br><br>We define it as maximize the return or the reward on the long-term.<br><br>And what is the reward is the,<br><br>the numbers that you have here,<br><br>that were defined by a human.<br><br>So, this is where the human defines the reward.<br><br>Now, what's the game? The game has five states.<br><br>State one is a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=trash can"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">trash can</span></a></object> and has a reward of plus two.<br><br>State two is a starting state, initial state,<br><br>and we assume that we would start in<br><br>the initial state with the plastic bottle in our hand.<br><br>The goal would be to throw this plastic bottle in a can.<br><br>[NOISE] If it hits the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=trash can"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">trash can</span></a></object>, we get +2.<br><br>If we get to state five,<br><br>we get to the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=recycle bin"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">recycle bin</span></a></object>,<br><br>and we can get +10.<br><br>Super important application.<br><br>State four has a chocolate.<br><br>So, what happens is if you go to state four,<br><br>you get a reward of 1 because you can eat the chocolate,<br><br>and you can also throw the,<br><br>the chocolate in the, in - in the,<br><br>in the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=recycle bin"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">recycle bin</span></a></object> hopefully.<br><br>Does this setting makes sense?<br><br>So, these states are of three types.<br><br>One is the starting state initial which is brown.<br><br>The [NOISE] normal state which is not a starting neither,<br><br>neither a starting nor a,<br><br>an ending state, and it's gray.<br><br>And the blue states are <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=terminal state"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">terminal states</span></a></object>.<br><br>So, if we get to the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=terminal state"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">terminal state</span></a></object>,<br><br>we end up a game or an episode let's say.<br><br>Does the setting makes sense?<br><br>Okay, and you have two, two possible actions.<br><br>You have to move. Either you go on the left or you go on the right.<br><br>An additional rule will,<br><br>will add is that the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=garbage collector"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">garbage collector</span></a></object> will come in three minutes,<br><br>and every step takes you one minute.<br><br>So, you cannot spend more than three minutes in this game.<br><br>In other words, you cannot stay at the chocolate and eat chocolate forever.<br><br>You have to, to move at some point.<br><br>Okay. So, one question I'd have is how do you define the long-term return?<br><br>Because we said we want a long-term return.<br><br>We don't want, we don't care about short-term returns.<br><br>[NOISE] What do you<br><br>think is a good way to define the long-term return here? [NOISE] Yeah.<br><br>Sum of the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=terminal state"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">terminal state</span></a></object>.<br><br>The sum of the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=terminal state"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">terminal states</span></a></object>.<br><br>No, the sum of how many points you have when you reach the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=terminal state"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">terminal state</span></a></object>.<br><br>The sum of how many points you have when you reach a terminal state.<br><br>So, let's say I'm in this state two,<br><br>I have zero reward right now.<br><br>If I reach <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the terminal"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the terminal</span></a></object> state on the,<br><br>on the, on your left, the +2.<br><br>I get +2 reward and I finish the game.<br><br>If I go on the right instead and I reached the +10.<br><br>You are saying that the long-term return can be<br><br>all the sum of the rewards I got to get there, so +11.<br><br>So, this is one way to define the long-term return.<br><br>Any other ideas?<br><br>[NOISE]<br><br>[<object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=inaudible"><span style="border-bottom:1px solid #FF0000;">inaudible</span></a></object>] reward.<br><br>Yeah, we probably want to incorporate the time-steps<br><br>and reduce the reward as, as time passes<br><br>and in fact this would be called a discounted return<br><br>versus what you said would be called a return.<br><br>Here we'll use a discounted return in and it has several advantages,<br><br>some are mathematical because the return<br><br>you describe which is not discounted might not <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=converge"><span style="border-bottom:1px solid #FF0000;">converge</span></a></object>.<br><br>It might go up to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=plus infinity"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">plus <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=infinity"><span style="border-bottom:1px solid #FF0000;">infinity</span></a></object></span></a></object>.<br><br>This discounted return will <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=converge"><span style="border-bottom:1px solid #FF0000;">converge</span></a></object> with an appropriate discount.<br><br>So <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=intuitively"><span style="border-bottom:1px solid #FF0000;">intuitively</span></a></object> also, why is the discounted return <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=intuitive"><span style="border-bottom:1px solid #FF0000;">intuitive</span></a></object>,<br><br>it's because time is always an important factor in our decision-making.<br><br>People prefer cash now then cash in 10 years, right?<br><br>Or similarly, you can consider that the robot has a limited <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=life expectancy"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">life <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=expectancy"><span style="border-bottom:1px solid #FF0000;">expectancy</span></a></object></span></a></object>,<br><br>like it has a battery and loses battery every time it moves.<br><br>So you want to take into account this discount of if I can eat chocolate close,<br><br>I go for it because I know that if the chocolate is too far,<br><br>I might not get there because I'm losing some battery,<br><br>some energy for example.<br><br>So this is the discounted return.<br><br>Now, if we take <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gamma"><span style="border-bottom:1px solid #FF0000;">gamma</span></a></object> equals one which means we have no discount,<br><br>the best strategy to follow in the settings seems to be to go<br><br>to the left or to go to the right starting in the initial state two.<br><br>Right. And the reason is,<br><br>it's a simple <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=computation"><span style="border-bottom:1px solid #FF0000;">computation</span></a></object>.<br><br>On one side I get +2,<br><br>on the other side I get +11.<br><br>What if my discount was 0.1?<br><br>Which one would be better?<br><br>Yeah, the left would be better, directly to plus.<br><br>And the reason is because we <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=compute"><span style="border-bottom:1px solid #FF0000;">compute</span></a></object> in our mind.<br><br>We just do 0 plus 0.1 times 1,<br><br>which gives us 0.1,<br><br>plus 0.1 squared times 10.<br><br>And it's less than 2. We know it.<br><br>Okay. So now we're going to assume that the discount is 0.9.<br><br>And it's a very common discount to<br><br>use in reinforcement learning and we use a discounted return.<br><br>So the general question here and it's the core<br><br>of reinforcement learning in this case of Q-Learning is,<br><br>what do we want to learn?<br><br>And this is really, really,<br><br>think of it as a human,<br><br>what would you like to learn?<br><br>What are the numbers you need to have<br><br>in order to be able to make decisions really quickly,<br><br>assuming you had a lot more states than that and actions?<br><br>Any ideas of what we want to learn?<br><br>What would help our decision-making?<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=optimal"><span style="border-bottom:1px solid #FF0000;">Optimal</span></a></object> action at each state.<br><br>Yeah. That's exactly what we want to learn.<br><br>For a given state, tell me the actions that I can take.<br><br>And for that I need to have a score for all the actions in every state.<br><br>In order to store the scores,<br><br>we need a matrix, right?<br><br>So, this is our matrix. We will call it a Q table.<br><br>It's going to be of shape,<br><br>number of states times number of actions.<br><br>If I have this matrix of scores and the scores are correct,<br><br>I'm in state three,<br><br>I can look on the third row of this matrix and look what's the maximum value I have,<br><br>is it the first one or the second one?<br><br>If it's the first one, I go to the left,<br><br>if it's the second one that is maximum, I go to the right.<br><br>This is what we would like to have.<br><br>Does that make sense, this Q table?<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=so now"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">So now</span></a></object>, let's try to build a Q table for this example.<br><br>If you had to build it, you would first think of it as a tree.<br><br>Oh and by the way, every entry of this Q table tells you how good<br><br>it is to take this action in that state.<br><br>State corresponding to the row,<br><br>action corresponding to the column.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=so now"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">So now</span></a></object>, how do we get there? We can build a tree.<br><br>And that's, that's similar to what we would do in our mind.<br><br>We start in S2. In S2, we have two options.<br><br>Either we go to S1,<br><br>we get 2, or we go to S3 and we get 0.<br><br>From S2 we - from S1,<br><br>we cannot go anywhere, it's a terminal state.<br><br>But from S3, we can go to S2,<br><br>and get 0 by going back,<br><br>or we can go to S4 and get 1.<br><br>That makes sense? From S4, same.<br><br>We can get 0 by going back to S3 or we can go to S5 and get +10.<br><br>Now, here I just have my immediate reward for every state.<br><br>What I would like <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=to compute"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=compute"><span style="border-bottom:1px solid #FF0000;">compute</span></a></object></span></a></object> is<br><br>the discounted return for all the state because ultimately,<br><br>what should lead my decision-making in a state is,<br><br>if I take this action,<br><br>I get to a new state.<br><br>What's the maximum reward I can get from there in the future?<br><br>Not just the reward I get in that state.<br><br>If I take the other action,<br><br>I get to another state.<br><br>What's the maximum reward I could get from that state?<br><br>Not just the immediate reward that I get from going to that state.<br><br>So what we will do - we can do it together.<br><br>Let's say we want <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=to compute"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=compute"><span style="border-bottom:1px solid #FF0000;">compute</span></a></object></span></a></object> the value of,<br><br>of the actions from S3,<br><br>from S3 going right to left.<br><br>From S3, I can either go to S4 or S2.<br><br>Going to S4, I know that the immediate reward was 1,<br><br>and I know that from S4,<br><br>I can get +10.<br><br>This is the maximum I can get.<br><br>So I can discount this 10 <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=multiply by"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">multiplied by</span></a></object> 0.9,<br><br>10 times 0.9 gives us 9,<br><br>+ 1 which was the immediate reward gives us 9,<br><br>gives us, gives us 10.<br><br>So 10 is the score that we give to the action go right from state S3.<br><br>Now, what if we do it from one step before S2?<br><br>From S2, I know that I can go to S3,<br><br>and to S3 I get zero reward.<br><br>So the immediate reward is zero.<br><br>But I know that from S3,<br><br>I can get 10 reward ultimately on the long-term.<br><br>I need to discount this reward from one step.<br><br>So I multiply this 10 by 0.9 and I get 0 plus 0.9 times 10 which gives me 9.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=so now"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">So now</span></a></object>, in state two going right will give us a long-term reward of 9.<br><br>Makes sense? And you do the same thing.<br><br>You can copy back that going from S4 to S3 will give you 0 plus the maximum,<br><br>you can get from S3 which was 10 discounted by 0.9,<br><br>or you can do it from S2.<br><br>From S2, I can go left and get +2,<br><br>or I can go right, and get 9.<br><br>And the immediate reward would be 9, would be 0.<br><br>And I will discount the 9 by 0.9 and get 8.1.<br><br>So that's the process we would do <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=to compute"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=compute"><span style="border-bottom:1px solid #FF0000;">compute</span></a></object></span></a></object> that.<br><br>And you see that it's an <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=iterative algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=iterative"><span style="border-bottom:1px solid #800000;">iterative</span></a></object> algorithm</span></a></object>.<br><br>I will just copy back all these values in my matrix.<br><br>And now, if I'm in state two,<br><br>I can clearly say that the best action seems to go,<br><br>seems to say go to the left because the long-term discounted reward is 9,<br><br>while the long-term discounted reward for going to the right is 2.<br><br>And I'm done. That's Q-Learning.<br><br>I solved the problem.<br><br>I had, I had a problem statement.<br><br>I found a matrix that tells me in every state what action I should take.<br><br>I'm done. So, why do we need deep learning?<br><br>That's the question we will try to answer.<br><br>So the best strategy to follow with 0.9 is still right,<br><br>right, right, and the way I see it is,<br><br>I just look at my matrix at every step.<br><br>And I follow always the maximum of my row.<br><br>So, from state two,<br><br>9 is the maximum, so I go right.<br><br>From state three, 10 is the maximum so I still go right.<br><br>And from state four, 10 is the maximum, so I go right again.<br><br>So I take the maximum over all the actions in a specific state.<br><br>Okay. Now, one interesting thing to follow is that when you do this <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=iterative algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=iterative"><span style="border-bottom:1px solid #800000;">iterative</span></a></object> algorithm</span></a></object>,<br><br>at some point, it should <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=converge"><span style="border-bottom:1px solid #FF0000;">converge</span></a></object>.<br><br>And ours <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=converged"><span style="border-bottom:1px solid #FF0000;">converged</span></a></object> to some values that<br><br>represent the discounted rewards for every state and action.<br><br>There is an equation that this Q-function follows.<br><br>And we know that the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=optimal"><span style="border-bottom:1px solid #FF0000;">optimal</span></a></object> Q-function follows this equation.<br><br>The one we have here follows this equation.<br><br>This equation is called the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bellman equation"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bellman"><span style="border-bottom:1px solid #800000;">Bellman</span></a></object> equation</span></a></object>.<br><br>And it has two terms.<br><br>One is R and one is discount times the maximum of the Q scores over all the actions.<br><br>So, how does that make sense?<br><br>Given that you're in a state S,<br><br>you want to know the score of going,<br><br>of taking action a in the state.<br><br>The score should be the reward that you get by going there,<br><br>plus the discount times the maximum you can get in the future.<br><br>That's actually what we used in the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=iteration"><span style="border-bottom:1px solid #FF0000;">iteration</span></a></object>.<br><br>Does this <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bellman equation"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bellman"><span style="border-bottom:1px solid #800000;">Bellman</span></a></object> equation</span></a></object> make sense?<br><br>Okay. So remember this is going to be<br><br>very important in Q-learning, this <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bellman equation"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bellman"><span style="border-bottom:1px solid #800000;">Bellman</span></a></object> equation</span></a></object>.<br><br>It's the equation that is satisfied by the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=optimal"><span style="border-bottom:1px solid #FF0000;">optimal</span></a></object> Q table or Q-function.<br><br>And if you try out all these entries,<br><br>you will see that it follows this equation.<br><br>So when Q didn't,<br><br>is not <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=optimal"><span style="border-bottom:1px solid #FF0000;">optimal</span></a></object>, it's not following this equation yet.<br><br>We would like you to follow this equation.<br><br>Another point of vocabulary in reinforcement learning is a policy.<br><br>Policies <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=denoted"><span style="border-bottom:1px solid #FF0000;">denoted</span></a></object> P sometimes or <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=mu"><span style="border-bottom:1px solid #FF0000;">mu</span></a></object>.<br><br>And - sorry.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pi"><span style="border-bottom:1px solid #FF0000;">Pi</span></a></object>, <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pi"><span style="border-bottom:1px solid #FF0000;">pi</span></a></object> of S is equal to argmax over the actions of the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=optimal"><span style="border-bottom:1px solid #FF0000;">optimal</span></a></object> Q that you have.<br><br>What it means it is exactly our decision process.<br><br>It's even that we are in state S. We look at<br><br>all the columns of the state S in our Q table, we take the maximum.<br><br>And this is what <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pi"><span style="border-bottom:1px solid #FF0000;">pi</span></a></object> of S is telling us.<br><br>It's telling us, "This is the action you should take."<br><br>So, <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pi"><span style="border-bottom:1px solid #FF0000;">pi</span></a></object>, our policy is our decision-making.<br><br>Okay. It tells us what's the best strategy to follow in a given state.<br><br>Any questions so far?<br><br>Okay, and so I have a question for you.<br><br>Why is deep learning helpful?<br><br>Yes?<br><br>The number of states is - is large, is way too large to store.<br><br>Yeah. That's very easy.<br><br>Number of states is way too large to store a table like that.<br><br>So like, if you have a small number of states and number of action,<br><br>then easily you can you use the Q-table.<br><br>You can add every state.<br><br>Looking into the Q-table is super quick,<br><br>and find out what you should do.<br><br>But ultimately, this Q-table will get bigger<br><br>and bigger depending on the application, right?<br><br>And the number of states for Go is 10 to the power 170 approximately,<br><br>which means that this matrix should have a number of rows<br><br>equal to 10 with 170 zeros after it.<br><br>You-you know what I mean. It's very big.<br><br>And number of actions is also going to be bigger.<br><br>In Go, you can place your action everywhere on the board that is available of course.<br><br>Okay. So, many - way too many states and actions.<br><br>So, we would need to come up with<br><br>maybe a function <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=approximator"><span style="border-bottom:1px solid #0000FF;">approximator</span></a></object> that can give us the action based on the state,<br><br>instead of having to store this matrix.<br><br>That's where deep learning will come.<br><br>So, just to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=recap"><span style="border-bottom:1px solid #800000;">recap</span></a></object> these first 30 minutes.<br><br>In terms of vocabulary, we learned what an environment is.<br><br>It's the - it's the general game definition.<br><br>An agent is the thing we're trying to train, the decision-maker.<br><br>A state, an action,<br><br>reward, total return, a discount factor.<br><br>The Q-table which is <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the matrix"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the matrix</span></a></object> of entries representing<br><br>how good it is to take action A in state S,<br><br>a policy which is our decision-making function,<br><br>telling us what's the best strategy to apply in a state.<br><br>and <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bellman equation"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bellman"><span style="border-bottom:1px solid #800000;">Bellman</span></a></object> equation</span></a></object> which is satisfied by the optimal Q - table.<br><br>Now, we will <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=tweak"><span style="border-bottom:1px solid #FF0000;">tweak</span></a></object> this Q-table into a Q-function.<br><br>And that's where we - we shift from Q-learning to deep Q-learning.<br><br>So, find a Q-function to replace the Q-table.<br><br>Okay? So, this is the setting.<br><br>We have our problem statement. We have our Q-table.<br><br>We want to change it into a function <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=approximator"><span style="border-bottom:1px solid #0000FF;">approximator</span></a></object> that will be our <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=neural network"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=neural"><span style="border-bottom:1px solid #FF0000;">neural</span></a></object> network</span></a></object>.<br><br>Does that make sense how deep learning comes into reinforcement learning here?<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=so now"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">So now</span></a></object>, we take a state as input, forward <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=propagate"><span style="border-bottom:1px solid #FF0000;">propagate</span></a></object> it in the deep network,<br><br>and get an output which is an action - an action score.<br><br>For all the actions.<br><br>It makes sense to have an output layer that is the size of the number of<br><br>actions because we don't wanna - we don't wanna<br><br>give an action as input and the state as input,<br><br>and get the score for this action taken in this state.<br><br>Instead, we can be much quicker.<br><br>We can just give the state as inputs,<br><br>get all the distribution of scores over the output,<br><br>and we just select the maximum of this vector,<br><br>which will tell us which action is best.<br><br>So if - if we're in state two let's say here.<br><br>We are in state two and we forward <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=propagate"><span style="border-bottom:1px solid #FF0000;">propagate</span></a></object> state two,<br><br>we get two values which are the scores of going left and right from state two.<br><br>We can select the maximum of those and it will give us our action.<br><br>The question is how to train this network? We know how to train it.<br><br>We've been learning it for nine weeks.<br><br>Compute the loss, back <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=propagate"><span style="border-bottom:1px solid #FF0000;">propagate</span></a></object>.<br><br>Can you guys think of some issues that,<br><br>that make this setting different from a classic supervised learning setting?<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the reward change is dynamic"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">The reward change is dynamic</span></a></object>.<br><br>Yes?<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the reward change is dynamic"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">The reward change is dynamic</span></a></object>.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the reward change is dynamic"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">The reward change is dynamic</span></a></object>.<br><br>So, the reward doesn't change. The reward is set.<br><br>You define it at the beginning. It doesn't change <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=dynamically"><span style="border-bottom:1px solid #800000;">dynamically</span></a></object>.<br><br>But I think what you meant is that the Q-scores change <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=dynamically"><span style="border-bottom:1px solid #800000;">dynamically</span></a></object>.<br><br>Yeah.<br><br>That's true. The Q-scores change <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=dynamically"><span style="border-bottom:1px solid #800000;">dynamically</span></a></object>.<br><br>But that's - that's probably okay because our network changed.<br><br>Our network is now the Q-score.<br><br>So, when we update the parameters of the network,<br><br>it updates the Q-scores.<br><br>What's-what's another issue that we might have?<br><br>No labels.<br><br>No labels. Remember in supervised learning,<br><br>you need labels to train your network. What are the labels, here?<br><br>[NOISE]. And don't say<br><br>compute the Q-table, use them as labels.<br><br>It's not gonna work. [NOISE]. Okay. So, that's<br><br>the main issue that makes this problem very different from classic supervised learning.<br><br>So, let's see how - how deep learning can be <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=tweaked"><span style="border-bottom:1px solid #FF0000;">tweaked</span></a></object> a little.<br><br>And we want you to see these techniques because they - they're<br><br>helpful when you read a variety of research papers.<br><br>We have our network given a state gives<br><br>us two scores that represent actions for going left and right from the state.<br><br>The loss function that we will define,<br><br>is it a classification problem or a regression problem?<br><br>[NOISE] Regression problem because<br><br>the Q-score doesn't have to be a probability between zero and one.<br><br>It's just a score that you want to give.<br><br>And that should look - that should me - meet the long-term discounted reward.<br><br>In fact, the loss function we can use is the L2 loss function,<br><br>y minus the Q-score squared.<br><br>So, let's say we do it for the Q going to the right.<br><br>The question is, what is y?<br><br>What is the target for this Q?<br><br>And remember what I copied on the top of the slide is the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bellman equation"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bellman"><span style="border-bottom:1px solid #800000;">Bellman</span></a></object> equation</span></a></object>.<br><br>We know that the optimal Q should follow this equation. We know it.<br><br>The problem is that this equation depends on its own Q.<br><br>You know like, you have Q on both sides of the equation.<br><br>It means if you set the label to be r plus <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gamma"><span style="border-bottom:1px solid #FF0000;">gamma</span></a></object> times <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the max"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the max</span></a></object> of Q stars,<br><br>then when you will back <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=propagate"><span style="border-bottom:1px solid #FF0000;">propagate</span></a></object>,<br><br>you will also have a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=derivative"><span style="border-bottom:1px solid #FF0000;">derivative</span></a></object> here.<br><br>Let me - let me go into the details.<br><br>Let's define a target value.<br><br>Let's assume that going, uh,<br><br>left is better than going right at this point in time.<br><br>So, we <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=initialize"><span style="border-bottom:1px solid #800000;">initialize</span></a></object> the network <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=randomly"><span style="border-bottom:1px solid #FF0000;">randomly</span></a></object>.<br><br>We forward <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=propagate"><span style="border-bottom:1px solid #FF0000;">propagate</span></a></object> state two in the network,<br><br>and the Q-score for left is more than the Q-score for right.<br><br>So, that's the action we will take at this point is going left.<br><br>Let's define our target y as the reward you get when you go left immediate,<br><br>plus <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gamma"><span style="border-bottom:1px solid #FF0000;">gamma</span></a></object> times the maximum of all the Q values you get from the next step.<br><br>So, let me spend a little more time on that because it's a little complicated.<br><br>I'm in s. I move to s next<br><br>using a move on the left?<br><br>I get immediate reward r,<br><br>and I also get a new state s prime, s next.<br><br>I can forward propagate this state in<br><br>the network and understand what is the maximum I can get from this state.<br><br>Take the maximum value and plug it in here.<br><br>So, this is hopefully what the optimal Q should follow.<br><br>It's a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=proxy"><span style="border-bottom:1px solid #FF0000;">proxy</span></a></object> to a good label.<br><br>It means we know that the Bellman equation tells us the best Q satisfies this equation.<br><br>But in fact, this equation is not true yet because the true equation we have Q star here,<br><br>not Q. Q star which is the optimal Q.<br><br>What we hope is that if we use this <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=proxy"><span style="border-bottom:1px solid #FF0000;">proxy</span></a></object> as our label,<br><br>and we learn the difference between where we are now in this <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=proxy"><span style="border-bottom:1px solid #FF0000;">proxy</span></a></object>,<br><br>we can then update the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=proxy"><span style="border-bottom:1px solid #FF0000;">proxy</span></a></object>,<br><br>get closer to the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=optimality"><span style="border-bottom:1px solid #800000;">optimality</span></a></object>.<br><br>Train again, update the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=proxy"><span style="border-bottom:1px solid #FF0000;">proxy</span></a></object>,<br><br>get closer to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=optimality"><span style="border-bottom:1px solid #800000;">optimality</span></a></object>,<br><br>train again, and so on.<br><br>Our only hope is that these will <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=converge"><span style="border-bottom:1px solid #FF0000;">converge</span></a></object>.<br><br>So, does it make sense how this is different from deep learning?<br><br>The labels are moving.<br><br>They're not static labels.<br><br>We define a label to be a best guess of what would be the best Q-function we have.<br><br>Then we compute the loss of where the Q-function is right now compared to this.<br><br>We back propagate so that the Q-function gets closer to our best guess.<br><br>Then, now that we have a better Q-function,<br><br>we can have a better guess.<br><br>So, we make a better guess,<br><br>and we fix this guess.<br><br>And now, we compute the difference between this Q-function that we have and our best guess.<br><br>We back propagate up.<br><br>We get to our best guess.<br><br>We can update our best guess again.<br><br>And we hope that doing that <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=iteratively"><span style="border-bottom:1px solid #800000;">iteratively</span></a></object> will end with the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=convergence"><span style="border-bottom:1px solid #FF0000;">convergence</span></a></object><br><br>and a Q-function that will be very close to satisfy the Bellman equation,<br><br>the optimal Bellman equation.<br><br>Does it make sense? This is the most complicated part of Q-learning. Yeah?<br><br>So, are you saying we generate [inaudible] of the Q-function?<br><br>We generate the output of the network,<br><br>we get the Q function,<br><br>we compare it to the Q,<br><br>the best Q function that we think is - it is.<br><br>What is the best Q function?<br><br>The one that satisfies the Bellman equation.<br><br>But we're never actually given the Bellman equation.<br><br>We don't but we - we guess it based on the Q we have.<br><br>Okay.<br><br>So basically, when you have Q you can compute<br><br>this Bellman equation and it will give you some values.<br><br>These values are probably closer to where you want to get,<br><br>to where you - from where you are now.<br><br>Where you are now is - is further from this <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=optimality"><span style="border-bottom:1px solid #800000;">optimality</span></a></object>,<br><br>and you want to reduce this gap by,<br><br>by - like to close the gap,<br><br>you back propagate. Yes?<br><br>Is there possibility for this will <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=diverge"><span style="border-bottom:1px solid #FF0000;">diverge</span></a></object>?<br><br>So, the question is,<br><br>is there a possibility for this to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=diverge"><span style="border-bottom:1px solid #FF0000;">diverge</span></a></object>?<br><br>So, this is a broader discussion that would take a full lecture to prove.<br><br>So, I put a paper here from<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=fran"><span style="border-bottom:1px solid #FF0000;">Fran</span></a></object> - <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=francisco"><span style="border-bottom:1px solid #FF0000;">Francisco</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=melo"><span style="border-bottom:1px solid #800000;">Melo</span></a></object> which proves the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=convergence"><span style="border-bottom:1px solid #FF0000;">convergence</span></a></object> of this algorithm so,<br><br>it <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=converges"><span style="border-bottom:1px solid #FF0000;">converges</span></a></object>, and in fact,<br><br>it <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=converges"><span style="border-bottom:1px solid #FF0000;">converges</span></a></object> because we're using a lot of tips and tricks that we will see later,<br><br>but if you want to see the math behind it and it's a,<br><br>it's a full lecture of proof,<br><br>I invite you to look at this simple ah proof for <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=convergence"><span style="border-bottom:1px solid #FF0000;">convergence</span></a></object> of the Bellman equation.<br><br>Okay. Okay. So, this is the case where our left score is higher than right score,<br><br>and we have two terms in our target,<br><br>immediate reward for taking action left and also discounted<br><br>maximum future reward when you are in state S, S next.<br><br>Okay. The - the, the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=tricky"><span style="border-bottom:1px solid #FF0000;">tricky</span></a></object> part is that let's say,<br><br>we we compute that we can do it, we have everything,<br><br>we have everything <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=to compute"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">to compute</span></a></object> our target,<br><br>we have R which is defined by the,<br><br>by the human at the beginning,<br><br>and we can also get this number because we know that if we<br><br>take action left we can then get S next,<br><br>and we forward propagate S next in the network.<br><br>We take the maximum output and it's this.<br><br>So, we have everything in this, in this equation.<br><br>The problem now is if I plug this and<br><br>my Q score in my loss function and I ask you to back propagate.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=back propagation"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">Back <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=propagation"><span style="border-bottom:1px solid #FF0000;">propagation</span></a></object></span></a></object> is what W equals W<br><br>minus alpha times the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=derivative"><span style="border-bottom:1px solid #FF0000;">derivative</span></a></object> of the loss function with respect to W,<br><br>the parameters of the network.<br><br>Which term will have a non-zero value?<br><br>Obviously, the second term Q of S go to the left will have<br><br>a non-zero value because it depends on the parameters of the network W,<br><br>but Y will also have a non-zero value.<br><br>Because you have Q here.<br><br>So, how do you handle that?<br><br>You actually get a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=feedback loop"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">feedback loop</span></a></object> in<br><br>this <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=back propagation"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">back <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=propagation"><span style="border-bottom:1px solid #FF0000;">propagation</span></a></object></span></a></object> that makes the network <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=unstable"><span style="border-bottom:1px solid #FF0000;">unstable</span></a></object>.<br><br>What we do is that we consider this fixed,<br><br>we will consider this Q fixed.<br><br>The Q that is our target is going to be fixed for many <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=iteration"><span style="border-bottom:1px solid #FF0000;">iteration</span></a></object>.<br><br>Let's say, a million or a 100,000 <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=iteration"><span style="border-bottom:1px solid #FF0000;">iteration</span></a></object> until we get close to there,<br><br>and our <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradient"><span style="border-bottom:1px solid #FF0000;">gradient</span></a></object> is small then we will update it and we'll fix it.<br><br>So, we actually have two networks in parallel,<br><br>one that is fixed and one that is not fixed.<br><br>Okay. And the second case is similar.<br><br>If the Q score to go on the right was more than the Q score to go on the left,<br><br>we would define our targets as immediate reward of going to<br><br>the right plus <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gamma"><span style="border-bottom:1px solid #FF0000;">gamma</span></a></object> times the maximum Q score we get,<br><br>if we're in the state that we will be in the next state and take the best action.<br><br>Does this makes sense? It's the most complicated part of Q-learning.<br><br>This is the hard part to understand.<br><br>So, immediate reward to go to<br><br>the right and discounted maximum future reward when you're in state S next, going to the right.<br><br>So, this is whole fix for <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=back prop"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">back prop</span></a></object>.<br><br>So, <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=no derivative"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">no <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=derivative"><span style="border-bottom:1px solid #FF0000;">derivative</span></a></object></span></a></object>. If we do that then no problem,<br><br>Y is just a number.<br><br>We come back to our original supervised learning setting.<br><br>Y is a number and we compute the loss and we back propagate, no difference.<br><br>Okay. So, compute <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=dl"><span style="border-bottom:1px solid #800000;">dL</span></a></object> - <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=dl"><span style="border-bottom:1px solid #800000;">dL</span></a></object> over <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=dw"><span style="border-bottom:1px solid #800000;">dW</span></a></object> and update W using <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=stochastic"><span style="border-bottom:1px solid #800000;">stochastic</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradient descent"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradient"><span style="border-bottom:1px solid #FF0000;">gradient</span></a></object> descent</span></a></object> methods.<br><br>RMS prop Adam whatever you guys want.<br><br>So, let's go over this, this full DQN,<br><br>deep Q-network implementation, and this slide is<br><br>a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pseudocode"><span style="border-bottom:1px solid #0000FF;">pseudocode</span></a></object> to help you understand how this entire algorithm works.<br><br>We will actually <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=plug in"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">plug in</span></a></object> many methods in this, in this <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pseudocode"><span style="border-bottom:1px solid #0000FF;">pseudocode</span></a></object>.<br><br>So, please focus right now,<br><br>and - and if you understand this,<br><br>you understand the entire rest of the lecture.<br><br>We <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=initialize"><span style="border-bottom:1px solid #800000;">initialize</span></a></object> our Q-network parameters just as we <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=initialize"><span style="border-bottom:1px solid #800000;">initialize</span></a></object> a network in deep learning,<br><br>we loop over episodes.<br><br>So, let's define an episode to be one game like going<br><br>from start to end to a terminal state, is one episode.<br><br>We can also define episode sometimes to be<br><br>many states like <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=breakout"><span style="border-bottom:1px solid #FF0000;">Breakout</span></a></object> which is the game with the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=paddle"><span style="border-bottom:1px solid #FF0000;">paddle</span></a></object>,<br><br>usually is 20 points.<br><br>The first player to get 20 points finishes the game.<br><br>So, episode will be 20 points.<br><br>Once your looping over episode starts from an initial state S. In our case,<br><br>it's only one initial state which is state two and loop over time steps.<br><br>Forward propagate S state two in the Q-network,<br><br>execute action A which has the maximum Q-score,<br><br>observe a immediate reward R and the next step S prime.<br><br>Compute target Y and to compute Y we know that,<br><br>we need to take S prime, forward propagate it in the network again,<br><br>and then, compute the loss function,<br><br>update the parameters with <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradient descent"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradient"><span style="border-bottom:1px solid #FF0000;">gradient</span></a></object> descent</span></a></object>.<br><br>Does this loop make sense?<br><br>It's very close to what we do in general.<br><br>The only difference would be this part<br><br>like we compute target Y using a double <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=forward propagation"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">forward <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=propagation"><span style="border-bottom:1px solid #FF0000;">propagation</span></a></object></span></a></object>.<br><br>So, with <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=forward propagation"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">forward <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=propagation"><span style="border-bottom:1px solid #FF0000;">propagation</span></a></object></span></a></object>,<br><br>we forward propagate two times in each loop.<br><br>Do you guys have any questions on,<br><br>on this <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pseudocode"><span style="border-bottom:1px solid #0000FF;">pseudocode</span></a></object>?<br><br>Okay. So, we will now see a concrete application of a Deep Q-Network.<br><br>So, this was the theoretical part.<br><br>Now, we are going to the practical part which is going to be more fun.<br><br>So, let's look at this game, it's called <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=breakout"><span style="border-bottom:1px solid #FF0000;">Breakout</span></a></object>.<br><br>The goal when you play <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=breakout"><span style="border-bottom:1px solid #FF0000;">Breakout</span></a></object> is to destroy all the bricks<br><br>without having the ball pass the line on the bottom.<br><br>So, we have a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=paddle"><span style="border-bottom:1px solid #FF0000;">paddle</span></a></object> and our decisions can be <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=idle"><span style="border-bottom:1px solid #FF0000;">idle</span></a></object>, stay,<br><br>stay where you are, move the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=paddle"><span style="border-bottom:1px solid #FF0000;">paddle</span></a></object> to the right or move the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=paddle"><span style="border-bottom:1px solid #FF0000;">paddle</span></a></object> to the left.<br><br>Right? And this <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=demo"><span style="border-bottom:1px solid #FF0000;">demo</span></a></object>,<br><br>and you have the credits on the bottom of the slide, uh,<br><br>shows that after training <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=breakout"><span style="border-bottom:1px solid #FF0000;">Breakout</span></a></object> using Q-learning they<br><br>gets a super intelligent agent which figures<br><br>out a trick to finish the game very quickly.<br><br>So, actually even like good players didn't know this trick,<br><br>professional players know this trick, but, uh,<br><br>in <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=breakout"><span style="border-bottom:1px solid #FF0000;">Breakout</span></a></object> you can actually try to dig a tunnel to get on the other side of the bricks,<br><br>and then, you will destroy all the bricks super<br><br>quickly from top to bottom instead of bottom-up.<br><br>What's super interesting is that the network figured out<br><br>this on its own without human supervision,<br><br>and this is the kind of thing we want.<br><br>Remember, if we were to use input, the Go boards and output professional players.<br><br>We will not figure out that type of stuff most of the time.<br><br>So, my question is,<br><br>what's the input of the Q-network in this setting?<br><br>Our goal is to destroy all the bricks. So, play Breakout.<br><br>What should be the input?<br><br>[NOISE]<br><br>Try something.<br><br>[inaudible] position of bricks.<br><br>Position of the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=paddle"><span style="border-bottom:1px solid #FF0000;">paddle</span></a></object>,<br><br>position of the bricks.<br><br>What else? Ball position.<br><br>Okay. Yeah I agree. So, this is what we will call a feature representation.<br><br>It means when you're in an environment you can extract some features, right?<br><br>And these are examples of features.<br><br>Giving the position of the ball is one feature,<br><br>giving the position of the bricks,<br><br>another feature, giving the position of the paddle, another feature.<br><br>Which are good features for this game,<br><br>but if you want to get the entire information you'd better do [NOISE] something else.<br><br>Yeah.<br><br>The <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pixels"><span style="border-bottom:1px solid #FF0000;">pixels</span></a></object>? You don't want any human supervision.<br><br>You don't wanna put features you - you just.<br><br>Okay. Take the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pixels"><span style="border-bottom:1px solid #FF0000;">pixels</span></a></object>,<br><br>take the game, you can control the paddle, take the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pixel"><span style="border-bottom:1px solid #FF0000;">pixel</span></a></object>.<br><br>Oh, yeah. This is a good input to the Q-network.<br><br>What's the output? I said it earlier.<br><br>Probably the output of the network will be 3<br><br>Q values representing the action going left,<br><br>going right and <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=stay idle"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">staying <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=idle"><span style="border-bottom:1px solid #FF0000;">idle</span></a></object></span></a></object> in a specific state.<br><br>That is the input of the network.<br><br>So, given a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pixel image"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pixel"><span style="border-bottom:1px solid #FF0000;">pixel</span></a></object> image</span></a></object> we want to predict Q scores for the three possible actions.<br><br>Now, what's the issue with that?<br><br>Do you think that would work or not?<br><br>Can someone think of something going wrong here?<br><br>Looking at the inputs.<br><br>[NOISE]<br><br>Okay. I'm gonna help you. If I give-yeah, you wanna try?<br><br>[inaudible].<br><br>Oh yeah, <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=good point"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">good point</span></a></object>. Based on this image,<br><br>you cannot know if the ball is going up or down.<br><br>Actually, it's super hard because the,<br><br>the action you take highly depends on whether the ball is going up or down, right?<br><br>If the ball is going down,<br><br>and even if the ball is going down,<br><br>you don't even know which direct - direction it's going down.<br><br>So, there is a problem here definitely.<br><br>There is not enough information to make a decision on the action to take.<br><br>And if it's hard for us,<br><br>it's gonna be hard for the network.<br><br>So, what's a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=hack to"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=hack"><span style="border-bottom:1px solid #FF0000;">hack</span></a></object> to</span></a></object>, to prevent that?<br><br>Is to take successive frames.<br><br>So, instead of one frame,<br><br>we can take four frames-successive frames.<br><br>And here, the same setting as we had before but we see that the ball is going up.<br><br>We see which direction is going up,<br><br>and we know what action we should take because we know the slope of the ball and also,<br><br>uh, also if it's going up or down.<br><br>That make sense? Okay. So, this is called the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=preprocessing"><span style="border-bottom:1px solid #0000FF;">preprocessing</span></a></object>.<br><br>Given a state, compute a function <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=phi"><span style="border-bottom:1px solid #800000;">Phi</span></a></object> of S. That gives<br><br>you the history of this state which is the four-sequence of four last frames.<br><br>What other <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=preprocessing"><span style="border-bottom:1px solid #0000FF;">preprocessing</span></a></object> can we do?<br><br>And this is something I want you to be quick.<br><br>Like, we we learned it together in deep learning, input <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=preprocessing"><span style="border-bottom:1px solid #0000FF;">preprocessing</span></a></object>.<br><br>Remember that second lecture where the question was what resolution should we use?<br><br>Remember, you have a cat recognition,<br><br>what resolution would you wanna use? Here same thing.<br><br>If we can reduce the size of the inputs, let's do it.<br><br>If we don't need all that information, let's do it.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=for example"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">For example</span></a></object>, do you think the colors are important?<br><br>Very minor. I don't think they're important.<br><br>So, maybe we can gray scale everything.<br><br>That removes three <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=chan"><span style="border-bottom:1px solid #FF0000;">chan</span></a></object> - that converts three channels into one channel,<br><br>which is amazing in terms of <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=computation"><span style="border-bottom:1px solid #FF0000;">computation</span></a></object>.<br><br>What else? I think we can crop a lot of these.<br><br>Like maybe there's a line here we don't need to make any decision.<br><br>We don't need the scores maybe.<br><br>So actually, there's some games where the score is important for decision-making.<br><br>The example is football like, or soccer.<br><br>Uh, when you're - when you're winning 1-0,<br><br>you you'd better if you're playing against a strong team defend like,<br><br>get back and defend to keep this 1-0.<br><br>So, the score is actually important in the decision-making process.<br><br>And in fact, uh, there are famous coach in<br><br>football which have this technique called park the bus,<br><br>where you just put all your team in front of the goal once you have scored a goal.<br><br>So, this is an example. So, here there is no park the bus.<br><br>But, uh, we can definitely get rid of the score,<br><br>which removes some <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pixels"><span style="border-bottom:1px solid #FF0000;">pixels</span></a></object> and reduces the number of <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=computations"><span style="border-bottom:1px solid #FF0000;">computations</span></a></object>,<br><br>and we can reduce to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=grayscale"><span style="border-bottom:1px solid #0000FF;">grayscale</span></a></object>.<br><br>One important thing to be careful about when you reduce to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=grayscale"><span style="border-bottom:1px solid #0000FF;">grayscale</span></a></object> is<br><br>that <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=grayscale"><span style="border-bottom:1px solid #0000FF;">grayscale</span></a></object> is a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=dimensionality reduction"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=dimensionality"><span style="border-bottom:1px solid #800000;">dimensionality</span></a></object> reduction</span></a></object> technique.<br><br>It means you - you lose information.<br><br>But you know, if you have three channels and you reduce everything in one channel,<br><br>sometimes you would have different color <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pixels"><span style="border-bottom:1px solid #FF0000;">pixels</span></a></object> which will end up with<br><br>the same <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=grayscale"><span style="border-bottom:1px solid #0000FF;">grayscale</span></a></object> value depending on the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=grayscale"><span style="border-bottom:1px solid #0000FF;">grayscale</span></a></object> that you use.<br><br>And it's been seen that you lose some information sometimes.<br><br>So, let's say the ball and some bricks have the same grayscale value,<br><br>then you would not differentiate them.<br><br>Or let's say the paddle and the background have the same grayscale value,<br><br>then you would not differentiate them.<br><br>So, you have to be careful of that type of stuff.<br><br>And there's other methods that do grayscale in other ways like <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=luminance"><span style="border-bottom:1px solid #800000;">luminance</span></a></object>.<br><br>So, we have our <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=phi"><span style="border-bottom:1px solid #800000;">Phi</span></a></object> of S which is this,<br><br>which is this uh,<br><br>input to the Q network,<br><br>and the deep Q network architecture is going to be<br><br>a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=convolutional"><span style="border-bottom:1px solid #0000FF;">convolutional</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=neural network"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=neural"><span style="border-bottom:1px solid #FF0000;">neural</span></a></object> network</span></a></object> because we're working with images.<br><br>So, we forward propagate that,<br><br>this is the architecture from Mnih, Kavukcuoglu,<br><br>Silver at al from Deep Mind.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=conv relu"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">CONV <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=relu"><span style="border-bottom:1px solid #0000FF;">ReLU</span></a></object></span></a></object>, <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=conv relu"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">CONV <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=relu"><span style="border-bottom:1px solid #0000FF;">ReLU</span></a></object></span></a></object>, <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=conv relu"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">CONV <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=relu"><span style="border-bottom:1px solid #0000FF;">ReLU</span></a></object></span></a></object>,<br><br>two fully connected layers and you get your Q-scores.<br><br>And we get back to our <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=training loop"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">training loop</span></a></object>.<br><br>So, what do we need to change in our <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=training loop"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">training loop</span></a></object> here?<br><br>Is we said that one frame is not enough.<br><br>So, we <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=preprocess"><span style="border-bottom:1px solid #0000FF;">preprocess</span></a></object> all the frames.<br><br>So, the initial state is converted to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=phi"><span style="border-bottom:1px solid #800000;">Phi</span></a></object> of<br><br>s. The <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=forward propagation"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">forward <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=propagation"><span style="border-bottom:1px solid #FF0000;">propagation</span></a></object></span></a></object> state is <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=phi"><span style="border-bottom:1px solid #800000;">Phi</span></a></object> of s and so on.<br><br>So, everywhere we had s or s prime,<br><br>we convert to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=phi"><span style="border-bottom:1px solid #800000;">Phi</span></a></object> of s or Phi of s prime which gives us the history.<br><br>Now, there are a lot more techniques that we can <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=plug in"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">plug in</span></a></object> here,<br><br>and we will see three more.<br><br>One is keeping track of <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the terminal"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the terminal</span></a></object> state.<br><br>In this loop, we should keep track of<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the terminal"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the terminal</span></a></object> state because we said if we reach a terminal state,<br><br>we want to end <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the loop"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the loop</span></a></object>, break <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the loop"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the loop</span></a></object>.<br><br>Now, the reason is because the y function.<br><br>So basically, we have to add,<br><br>create a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=boolean"><span style="border-bottom:1px solid #800000;">Boolean</span></a></object> to detect <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the terminal"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the terminal</span></a></object> state before <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=loop through"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">looping through</span></a></object> the time steps.<br><br>And inside <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the loop"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the loop</span></a></object>,<br><br>we wanna check if the new s prime we are going to is a terminal state.<br><br>If it's a terminal state,<br><br>then I can stop this loop and go back, play another episode.<br><br>So, play another, start at another starting state, and continue my game.<br><br>Now, this y target that we compute is different if we are in a terminal state or not.<br><br>Because if we're in a terminal state,<br><br>there is no reason to have a discounted long-term reward.<br><br>There's nothing behind that terminal state.<br><br>So, if we're in a terminal state, we just set it to the immediate reward and we break.<br><br>If we're not in a terminal state,<br><br>then we would add this discounted future reward.<br><br>Any questions on that?<br><br>Yep, another issue that we're seeing this and which makes, uh,<br><br>this reinforcement learning setting super different from the classic<br><br>supervised learning setting is that we only train on what we explore.<br><br>It means I'm starting in a state s. I compute,<br><br>I forward propagate this Phi of s in my network.<br><br>I get my vector of Q-values.<br><br>I select the best Q-value, the largest.<br><br>I get a new state because I can move now from state s to s prime.<br><br>So, I have a transition from s take action A,<br><br>get s prime or Phi of s. Take action A, get Phi of s prime.<br><br>Now, this is, is what I will use to train my network.<br><br>I can forward propagate Phi of s prime again in the network,<br><br>and get my y target.<br><br>Compare my y to my Q and then <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=backpropagate"><span style="border-bottom:1px solid #0000FF;">backpropagate</span></a></object>.<br><br>The issue is I may never explore this state transition again.<br><br>Maybe I will never get there anymore.<br><br>It's super different from what we do in supervised learning where you have a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=dataset"><span style="border-bottom:1px solid #FF0000;">dataset</span></a></object>,<br><br>and your <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=dataset"><span style="border-bottom:1px solid #FF0000;">dataset</span></a></object> can be used many times.<br><br>With batch <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradient descent"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradient"><span style="border-bottom:1px solid #FF0000;">gradient</span></a></object> descent</span></a></object> or with any <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradient descent algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradient"><span style="border-bottom:1px solid #FF0000;">gradient</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=descent algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">descent algorithm</span></a></object></span></a></object>.<br><br>One <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=epoch"><span style="border-bottom:1px solid #FF0000;">epoch</span></a></object>, you see all the data points.<br><br>So, if you do two <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=epochs"><span style="border-bottom:1px solid #FF0000;">epochs</span></a></object>, you see every data point two times.<br><br>If you do 10 <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=epochs"><span style="border-bottom:1px solid #FF0000;">epochs</span></a></object>, you see every data points three times or 10 times.<br><br>So, it means that every data point can be used several time to<br><br>train your algorithm in classic deep learning that we've seen together.<br><br>In this case, it doesn't seem possible because we only train when we explore,<br><br>and we might never get back there.<br><br>Especially because the training will be influenced by where we go.<br><br>So, maybe there are some places where we will never<br><br>go because while we train and while we learn,<br><br>it will, it will kind of direct our<br><br>decision process and we will never train on some parts of the game.<br><br>So, this is why we have other techniques to keep this training stable.<br><br>One is called experience <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=replay"><span style="border-bottom:1px solid #FF0000;">replay</span></a></object>.<br><br>So, as I said, here's what we're currently doing.<br><br>We have Phi of s, forward propagate, get a.<br><br>From taking action a,<br><br>we observe an immediate reward r,<br><br>and a new state Phi of s prime.<br><br>Then from Phi of s prime,<br><br>we can take a new action a prime,<br><br>observe a new reward r prime,<br><br>and the new state Phi of s prime prime, and so on.<br><br>And each of these is called a state transition,<br><br>and can be used to train.<br><br>This is one experience leads to one <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=iteration"><span style="border-bottom:1px solid #FF0000;">iteration</span></a></object> of <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradient descent"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">gradient descent</span></a></object>.<br><br>E1, E2, E3,<br><br>Experience 1, Experience 2, Experience 3.<br><br>And the training will be trained on Experience 1<br><br>then trained on Experience 2 then trained on Experience 3.<br><br>What we're doing with experience <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=replay"><span style="border-bottom:1px solid #FF0000;">replay</span></a></object> is the following.<br><br>We will observe experience 1 because we start in a state,<br><br>we take an action.<br><br>We see another state and a reward and this is called experience 1.<br><br>We will create a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=replay"><span style="border-bottom:1px solid #FF0000;">replay</span></a></object> memory.<br><br>You can think of it as a data structure in computer science<br><br>and you will place this Experience 1 <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=tuple"><span style="border-bottom:1px solid #800000;">tuple</span></a></object> in this <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=replay"><span style="border-bottom:1px solid #FF0000;">replay</span></a></object> memory.<br><br>Then from there, we will experience Experience 2.<br><br>We'll put Experience 2 in the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=replay"><span style="border-bottom:1px solid #FF0000;">replay</span></a></object> memory.<br><br>Same with Experience 3, put it in the replay memory and so on.<br><br>Now, during training, what we will do is we will first train on Experience<br><br>1 because it's the only experience we have so far.<br><br>Next step, instead of training on E 2,<br><br>we will train on a sample from E 1 and E 2.<br><br>It means we will take one out of the replay memory and use this one for training.<br><br>But we will still continue to experiment something else and we will sample from there.<br><br>And at every step,<br><br>the replay memory will become bigger and bigger and while we train,<br><br>we will not necessarily train on the step we explore,<br><br>we will train on a sample which is the replay memory plus the new state we explored.<br><br>Why is it good is because E 1 as you see can be useful<br><br>many times in the training and maybe one was a critical state.<br><br>Like it was a very important data point to learn or a Q function and so on and so on.<br><br>Does the replay memory makes sense?<br><br>So, several advantages.<br><br>One is data efficiency.<br><br>We can use data many times.<br><br>Don't have to use one data point only one time.<br><br>Another very important advantage of<br><br>experience replay is that if you don't use experience replay,<br><br>you have a lot of correlation between the successive data points.<br><br>So, let's say the ball is on the bottom right here,<br><br>and the ball is going to the top left.<br><br>For the next 10 data points,<br><br>the ball is always going to go to the top left.<br><br>And it means the action you can take,<br><br>is always the same.<br><br>It actually doesn't matter a lot because the ball is going up.<br><br>But most likely you wanna follow where the ball is going.<br><br>So, the action will be to go towards the ball for 10 actions in a row.<br><br>And then the ball would bounce on the wall and on the top and go back down here,<br><br>down to the bottom left-to the bottom right.<br><br>What will happen if your paddle is here is that,<br><br>for 10 steps in a row you will send your paddle on the right.<br><br>Remember what we said when we,<br><br>when we ask the question if you have to train a cat<br><br>versus dog <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=classifier"><span style="border-bottom:1px solid #800000;">classifier</span></a></object> with batches of images of cats,<br><br>batches of images of dog, train first on the cats then trains on the dogs,<br><br>then trains on the cats, then trains on the dogs.<br><br>We will not <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=converge"><span style="border-bottom:1px solid #FF0000;">converge</span></a></object> because your network will be super<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bias towards"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">biased towards</span></a></object> predicting cat after seeing 10 images of cat.<br><br>Super biased with predicting dogs when it sees 10 images of dog.<br><br>That what's happening here.<br><br>So, you wanna de-correlate all these experiences.<br><br>You want to be able to take one experience,<br><br>take another one that has nothing to do with it and so on.<br><br>This is what experience replay does.<br><br>And the third one, is that the third one is that you're<br><br>basically trading <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=computation"><span style="border-bottom:1px solid #FF0000;">computation</span></a></object> and memory against exploration.<br><br>Exploration is super costly.<br><br>The state-space might be super big,<br><br>but you know you have enough <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=computation"><span style="border-bottom:1px solid #FF0000;">computation</span></a></object> probably,<br><br>you can have a lot of <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=computation"><span style="border-bottom:1px solid #FF0000;">computation</span></a></object> and you have memory space,<br><br>let's use an experience replay.<br><br>Okay. So, let's add experience replay to our code here.<br><br>The transition resulting from this part,<br><br>is added to the experience to<br><br>the replay memory D and will not necessarily be used in the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=iteration"><span style="border-bottom:1px solid #FF0000;">iteration</span></a></object> space.<br><br>So, what's happening is before we propagate phi of S,<br><br>we get, we observe a reward and an action.<br><br>And this action leads to a state phi of S prime.<br><br>This is an experience.<br><br>Instead of training on this experience,<br><br>I'm just going to take it, put it in the replay memory.<br><br>Add experience to replay memory.<br><br>And what I will train on is not this experience,<br><br>it's a sampled random mini-batch of transition from the replay memory.<br><br>So, you see, we're exploring but we're not training on what we explore.<br><br>We're training on the replay memory,<br><br>but the replay memory is dynamic.<br><br>It changes. And update using the sample transitions.<br><br>So, the sample transition from the replay memory will<br><br>be used to do the update. That's the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=hack"><span style="border-bottom:1px solid #FF0000;">hack</span></a></object>.<br><br>Now, another <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=hack"><span style="border-bottom:1px solid #FF0000;">hack</span></a></object> we want,<br><br>the last <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=hack"><span style="border-bottom:1px solid #FF0000;">hack</span></a></object> we want to talk about is exploration versus exploitation.<br><br>So, as a human, and let's say you're <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=commuting"><span style="border-bottom:1px solid #FF0000;">commuting</span></a></object> to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=stanford"><span style="border-bottom:1px solid #FF0000;">Stanford</span></a></object><br><br>every day and you know the road you're <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=commuting"><span style="border-bottom:1px solid #FF0000;">commuting</span></a></object> at. You know it.<br><br>You always take the same road and you're <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bias towards"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">biased towards</span></a></object> taking this road.<br><br>Why? Because the first time you took it it went well.<br><br>And the more you take it,<br><br>the more you learn about it not that it's good to know<br><br>the tricks of how to drive fast but but like you know the tricks,<br><br>you know that this, this,<br><br>this light is going to be green at that moment and so on.<br><br>So, you, you, you build a very good expertise in this road, super expert.<br><br>But maybe there's another road that you don't wanna try that is better.<br><br>You just don't try it because you're focused on that road.<br><br>You're doing exploitation.<br><br>You exploit what you already know.<br><br>Exploration would be - okay let's do it.<br><br>I'm gonna try another road today,<br><br>I might get late to the course but maybe I will have<br><br>a good discovery and I would like this road and I will take it later on.<br><br>There is a trade off between these two because the RL algorithm is going to<br><br>figure out some strategies that are super good.<br><br>And will try to do local search in these to get better and better.<br><br>But you might have another minimum that is better than this one and you don't explore it.<br><br>Using <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the algorithm</span></a></object> we currently have,<br><br>there's no trade-off between exploitation and exploration.<br><br>We're almost doing only exploitation.<br><br>So, how to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=incentivize"><span style="border-bottom:1px solid #0000FF;">incentivize</span></a></object> this exploration. Do you guys have an idea?<br><br>So, right now, when we're in a state S,<br><br>we're forward <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=propagating"><span style="border-bottom:1px solid #FF0000;">propagating</span></a></object> the states,<br><br>for all states in the network and we take the action that is the best action always.<br><br>So we're exploiting. We're exploiting what we already know. We take the best action.<br><br>Instead of taking this best action,<br><br>what can we do? Yeah.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=monte carlo"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=monte"><span style="border-bottom:1px solid #FF0000;">Monte</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=carlo"><span style="border-bottom:1px solid #FF0000;">Carlo</span></a></object></span></a></object> sampling.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=monte carlo"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=monte"><span style="border-bottom:1px solid #FF0000;">Monte</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=carlo"><span style="border-bottom:1px solid #FF0000;">Carlo</span></a></object></span></a></object> sampling, <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=good point"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">good point</span></a></object>. Another one, you wanted to try something else?<br><br>Could have a parameter that's<br><br>the ratio times you take the best action versus exploring another action.<br><br>Okay. Take a hyper-parameter that tells you when you can explore,<br><br>when you can exploit.<br><br>Is that what you mean?<br><br>Yeah, that's a good point.<br><br>So, I think that that's a solution.<br><br>You can take a hyper-parameter that is a probability telling<br><br>you with this probability explore,<br><br>otherwise with one minus this probability exploit.<br><br>That's what, that's what we're going to do.<br><br>So, let's look why exploration versus exploitation doesn't work.<br><br>We're in initial state one, S1.<br><br>And we have three options.<br><br>Either we go using action A1 to S2 and we get a reward of zero,<br><br>or we go to action use action 2,<br><br>get to S3 and get reward of 1 or use action 3 and go to S4,<br><br>and get a reward of 1,000.<br><br>So, this is obviously where we wanna go.<br><br>We wanna go to S4 because it has the maximum reward.<br><br>And we don't need to do much computation in our head.<br><br>It's simple, there is no discount, it's direct.<br><br>Just after <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=initializing"><span style="border-bottom:1px solid #800000;">initializing</span></a></object> the Q-network,<br><br>you get the following Q-values.<br><br>Forward propagate S1 in the Q-network and get 0.5 for taking action 1,<br><br>0.4 for taking action 2,<br><br>0.3 for taking action 3.<br><br>So, this is obviously not good but our network,<br><br>it was <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=randomly"><span style="border-bottom:1px solid #FF0000;">randomly</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=initialized"><span style="border-bottom:1px solid #800000;">initialized</span></a></object>.<br><br>What it's telling us is that 0.5 is the maximum.<br><br>So, we should take action 1. So, let's go.<br><br>Take action 1, observe S 2.<br><br>You observe a reward of 0.<br><br>Our target because it's a terminal state is only equal to the reward.<br><br>There is no additional term.<br><br>So, we want our target to match our Q. Our target is zero.<br><br>So, Q should match zero.<br><br>So, we train and we get the Q that should be zero.<br><br>Does that makes sense?<br><br>Now, we do another round of iteration.<br><br>We look we are in S1,<br><br>we get back to the beginning of the episode we see that<br><br>our Q function tells us that action two is the best.<br><br>Because 0.4 is the maximum value.<br><br>It means go to S3.<br><br>I go to S3, I observe reward of 1.<br><br>What does it mean? It's a terminal state.<br><br>So, my target is 1.<br><br>Y equals 1. I want the Q to match my Y.<br><br>So, my Q should be 1.<br><br>Now, I continue third step up.<br><br>Q function says go to A2.<br><br>I go to A2 nothing happens.<br><br>I already matched the reward.<br><br>Four step go to A2, you see what happens?<br><br>We will never go there.<br><br>We'll never get there because we're not exploring.<br><br>So, instead of doing that, what we are saying is that five percent of the time,<br><br>take your random action to explore and 95 percent of the time follow your exploitation.<br><br>Okay. So, that's where we add it.<br><br>With probability <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=epsilon"><span style="border-bottom:1px solid #800000;">epsilon</span></a></object>, the hyper-parameter take random action A,<br><br>otherwise do what we were doing before,<br><br>exploit. Does that make sense?<br><br>Okay, cool.<br><br>So, now we plugged in<br><br>all these tricks in our <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pseudo code"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pseudo"><span style="border-bottom:1px solid #800000;">pseudo</span></a></object> code</span></a></object> and this is our new <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pseudo code"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pseudo"><span style="border-bottom:1px solid #800000;">pseudo</span></a></object> code</span></a></object>.<br><br>So, we have <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=to initialize"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=initialize"><span style="border-bottom:1px solid #800000;">initialize</span></a></object></span></a></object> a replay memory which we did not have to do earlier.<br><br>In blue, you can find the replay memory added lines.<br><br>In orange, you can find the added lines for checking <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the terminal"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the terminal</span></a></object> state and in purple,<br><br>you can find the added lines related to epsilon-greedy,<br><br>exploration versus exploitation.<br><br>And finally <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=in bold"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">in bold</span></a></object>, the pre-processing.<br><br>Any questions on that?<br><br>So, that's, that's we wanted to see a variant<br><br>of how deep learning can be used in a setting<br><br>that is not necessarily classic supervised learning setting.<br><br>Can you see that the main advantage of deep learning in<br><br>this case is it's a good function <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=approximator"><span style="border-bottom:1px solid #0000FF;">approximator</span></a></object>?<br><br>Your <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=convolutional"><span style="border-bottom:1px solid #0000FF;">convolutional</span></a></object> neural network can extract a lot of information from<br><br>the pixels that we're not able to get with other networks.<br><br>Okay. So, let, let's see what we have here.<br><br>We have our super <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=atari"><span style="border-bottom:1px solid #800000;">Atari</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bot"><span style="border-bottom:1px solid #FF0000;">bot</span></a></object> that's gonna dig a tunnel,<br><br>and it's going to destroy all the bricks super quickly.<br><br>It's good to see that after building it.<br><br>So, this is work from DeepMind's team,<br><br>and you can find this video on <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=youtube"><span style="border-bottom:1px solid #0000FF;">YouTube</span></a></object>.<br><br>Okay, another thing I wanted to say quickly is<br><br>what's the difference between with and without human knowledge?<br><br>You will see a lot of people-a lot of papers mentioning<br><br>that this algorithm was trained with human learned knowledge,<br><br>or this algorithm was trained without any human <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=in the loop"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">in <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the loop"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the loop</span></a></object></span></a></object>.<br><br>Why is human knowledge very important?<br><br>Like, think about it.<br><br>Just playing one game as<br><br>a human and teaching that to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the algorithm</span></a></object> will help <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the algorithm</span></a></object> a lot.<br><br>When the algorithm sees this game,<br><br>what it sees is pixels.<br><br>What do we see when we see that game?<br><br>We see that there is a key here.<br><br>We know the key is usually a good thing.<br><br>So, we have a lot of context, right?<br><br>As a human. We know I'm probably gonna go for the key.<br><br>I'm not gonna go for this-this thing, no.<br><br>Uh, same, ladder. What is the ladder?<br><br>We-we directly identify that the ladder is something we can go up and down.<br><br>We identify that this rope is probably<br><br>something I can use to jump from one side to the other.<br><br>So as a human, there is a lot more background information that<br><br>we have even without knowing it-without realizing it.<br><br>So, there's a huge difference between<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=algorithms"><span style="border-bottom:1px solid #FF0000;">algorithms</span></a></object> trained with human <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=in the loop"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">in <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the loop"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the loop</span></a></object></span></a></object> and without human <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=in the loop"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">in the loop</span></a></object>.<br><br>This game is actually <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=montezuma's revenge"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">Montezuma's Revenge</span></a></object>.<br><br>The DQN algorithm when the paper came out on-on the<br><br>nature-on nature - in Nature the-the second-the second version of the paper.<br><br>They showed that they-they beat human on<br><br>49 games that are the same type of games I-as Breakout.<br><br>But this one was the hardest one.<br><br>So, they couldn't beat human on this one. And the reason was because<br><br>there's a lot of information and also the game has-is very long.<br><br>So in order-it's called <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=montezuma's revenge"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">Montezuma's Revenge</span></a></object> and.<br><br>I think Ramtin Keramati is going to talk about it a little later.<br><br>But in order to get to win this game,<br><br>you have to go through a lot of different stages,<br><br>and it's super long.<br><br>So, it's super hard for the-the-the algorithm to explore all the state space.<br><br>Okay. So, that slide I will show you<br><br>a few more games that-that the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=deepmind"><span style="border-bottom:1px solid #0000FF;">DeepMind</span></a></object> team has solved. <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=pong"><span style="border-bottom:1px solid #800000;">Pong</span></a></object> is one.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=seaquest"><span style="border-bottom:1px solid #0000FF;">SeaQuest</span></a></object> is another one,<br><br>and <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=space invader"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">Space <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=invaders"><span style="border-bottom:1px solid #FF0000;">Invaders</span></a></object></span></a></object> that you might know which-which is probably<br><br>the-the most famous of the three. I hope you know.<br><br>Okay. So, that said,<br><br>I'm gonna hand in the microphone to-we're lucky to have an RL expert.<br><br>So,  Ramtin Keramati is a fourth year <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=phd"><span style="border-bottom:1px solid #FF0000;">PhD</span></a></object> student,<br><br>uh, in RL working with Professor <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=brunskill"><span style="border-bottom:1px solid #0000FF;">Brunskill</span></a></object> at <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=stanford"><span style="border-bottom:1px solid #FF0000;">Stanford</span></a></object>.<br><br>And he will tell us a little bit about his experience and he will show<br><br>us some advanced applications of deep learning in RL,<br><br>and how these plug in together.<br><br>Thank you. Thanks <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=kian"><span style="border-bottom:1px solid #0000FF;">Kian</span></a></object> for that introduction.<br><br>Okay. Can everyone hear me now?<br><br>Right, good. Cool. Okay first,<br><br>I have like, 8-9 minutes.<br><br>You have more.<br><br>I have more?<br><br>Yes.<br><br>Okay. Great, okay first question.<br><br>After seeing that lecture so far like,<br><br>how many are you-of you are thinking that RL is actually cool? Like, honestly.<br><br>That's like, oh that's a lot.<br><br>Okay. [LAUGHTER] That's a lot.<br><br>Okay. My hope is after showing you some other advanced topics here,<br><br>then the percentage is gonna even increase.<br><br>So, let's [LAUGHTER] let's see.<br><br>Uh, it's almost impossible to talk about it like,<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=advancements"><span style="border-bottom:1px solid #FF0000;">advancements</span></a></object> in RL recently without mentioning Alpha Go<br><br>I think  right now wrote down on a table that it's almost 10 to the uh,<br><br>power of 170 different configuration of the board.<br><br>And that's roughly more<br><br>than-I mean that's more than the estimated number of atoms in the universe.<br><br>So, one traditional al-algorithm that before like deep learning and stuff like that.<br><br>Was like tree search in RL,<br><br>which is basically go<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=exhaustively"><span style="border-bottom:1px solid #800000;">exhaustively</span></a></object> search all the-all the possible actions that you can take,<br><br>and then take the best one.<br><br>In that situation Alpha Go that's all-all almost impossible.<br><br>So, what they do that's also a paper from <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=deepmind"><span style="border-bottom:1px solid #0000FF;">DeepMind</span></a></object> is that they<br><br>train a neural network for-for that.<br><br>They kind of merge the tree search with-with deep learning, a neural network that they have.<br><br>They have two kinds of networks.<br><br>One is called the value network.<br><br>Value network is basically consuming this image-image of<br><br>a board and telling you what's the probability that if you can win in this situation.<br><br>So, if the value is higher,<br><br>then the probability of winning is higher.<br><br>Oh, how does it help you-help you in the case that if you wanna search for the action,<br><br>you don't have to go until the end of the game<br><br>because the end of the game is a lot of steps,<br><br>and it's almost impossible to go to the end of the game in all these simulations.<br><br>So, that helps you to understand what's the value of each game like, <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=beforehand"><span style="border-bottom:1px solid #FF0000;">beforehand</span></a></object>?<br><br>Like, after 48 step or 58 step<br><br>if you're gonna win that game or-or if you're gonna lose that game?<br><br>There's another network of the policy network which helps us to take action.<br><br>But I think the most interesting thing of the Alpha Go is that it's trained <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=from scratch"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">from scratch</span></a></object>.<br><br>So, it trains from nothing,<br><br>and they have a tree called self play that-there is two AI playing with each other.<br><br>The best one I-replicate the best-the best one you keep it fixed,<br><br>and I have another one that is trying to beat the previous version of itself.<br><br>And after it can beat the previous version of itself like,<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=reliably"><span style="border-bottom:1px solid #FF0000;">reliably</span></a></object> many times, then I replace this<br><br>again for the previous one. And then I adjust it.<br><br>So, this is a training curve of like itself-a self play of the Alpha Go as you see.<br><br>And it takes a lot of compute.<br><br>So, that's kind of crazy.<br><br>But finally they beat the human.<br><br>Okay. Another type of algorithm, and this is like,<br><br>the whole different <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=class of algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">class of algorithm</span></a></object> called policy <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradients"><span style="border-bottom:1px solid #FF0000;">gradients</span></a></object>. Uh -.<br><br>We have developed <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=an algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">an algorithm</span></a></object> called trust region policy <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=optimization"><span style="border-bottom:1px solid #FF0000;">optimization</span></a></object>.<br><br>Can I stop that? [LAUGHTER].<br><br>This method was <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=abled"><span style="border-bottom:1px solid #0000FF;">abled</span></a></object> when <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=locomotion"><span style="border-bottom:1px solid #800000;">locomotion</span></a></object> controllers for [OVERLAPPING].<br><br>Can you <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=mute"><span style="border-bottom:1px solid #FF0000;">mute</span></a></object> the sound please?<br><br>Okay, great. So, policy <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradient algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">gradient algorithm</span></a></object>.<br><br>[LAUGHTER].<br><br>Well, what I can do is <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=restart"><span style="border-bottom:1px solid #FF0000;">restart</span></a></object> this from here. Uh -<br><br>No. That is not. Doesn't work.<br><br>Okay.<br><br>Okay. So, here like in the DQN that you've seen,<br><br>uh, you-you came and like,<br><br>compute the Q-value of each state.<br><br>And then what you have done is that you take the argmax of this with<br><br>respect to action and then you choose the action that you want to choose, right?<br><br>But what you care at the end of the day is the action<br><br>which is the mapping from a state to action,<br><br>which we call it a policy, right?<br><br>So, what you care at the end of the day is actually the policy.<br><br>Like, what action should they take?<br><br>Is not really Q value itself, right?<br><br>So, this class of - class of methods that is called policy <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=gradients"><span style="border-bottom:1px solid #FF0000;">gradients</span></a></object>,<br><br>is trying to directly <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=optimize for"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=optimize"><span style="border-bottom:1px solid #FF0000;">optimize</span></a></object> for</span></a></object> the policy.<br><br>So, rather than updating the Q function,<br><br>I compute the gradient of my policy.<br><br>I update my policy network again, and again, and again.<br><br>So, let's see these videos.<br><br>So, this is like this guy,<br><br>that is trying to reach the pink uh,<br><br>ball over there, and sometimes like gets hit by the some external forces.<br><br>And this is called uh, a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=raster algorithm"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=raster"><span style="border-bottom:1px solid #800000;">raster</span></a></object> algorithm</span></a></object>, call it like PPO.<br><br>This is a policy gradient.<br><br>I try to reach for that ball.<br><br>So, I think that you've heard of, ah, OpenAI<br><br>like five, like the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bot"><span style="border-bottom:1px solid #FF0000;">bot</span></a></object> that is playing DOTA.<br><br>So, this is like,<br><br>completely like, PPO algorithm.<br><br>And they have like,<br><br>a lot of compute showing that,<br><br>and I guess I have the numbers here.<br><br>There is 180 years of play in one day.<br><br>This is how much compute they have. Uh, so that's fine.<br><br>There's another even funnier video.<br><br>Its called Competitive Self-Play.<br><br>Again, the same idea with policy gradient.<br><br>Inside you put two agents in front of each other,<br><br>and they just try to beat each other.<br><br>And if they beat each other, they get a reward.<br><br>The most interesting part is that - for example in that game,<br><br>the purpose is just to-to pull the other one out, right?<br><br>But they understand some emerging behavior which is if -<br><br>for us human makes sense but for them to learn out of nothing is kind of cool.<br><br>[NOISE]<br><br>So there's like one risk here that when they're playing,<br><br>oh, this, uh, this guy's trying to kick the ball inside.<br><br>One, one risk here is to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=overfit"><span style="border-bottom:1px solid #0000FF;">overfit</span></a></object>.<br><br>[LAUGHTER] That's also cool.<br><br>[LAUGHTER] Oh, yeah.<br><br>One technical point before we move,<br><br>one technical point here is that here,<br><br>wait, where is the, no, the next one.<br><br>Okay. Here that two,<br><br>our two agent are p - p - playing with each other,<br><br>and we are just updating the person with the best other agent, like previously,<br><br>we are doing like a self-play,<br><br>is that you <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=overfit"><span style="border-bottom:1px solid #0000FF;">overfit</span></a></object> to the actual agent that you have in front of you.<br><br>So, uh, the agent in front of you is powerful,<br><br>but you might <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=overfit"><span style="border-bottom:1px solid #0000FF;">overfit</span></a></object> to this,<br><br>and if I, uh,<br><br>put the agent that is not that powerful but is<br><br>using this simple trick that the powerful agent,<br><br>like, never uses, then you might just l - lose the game, right?<br><br>So, one trick here to make it more stable is that<br><br>rather than playing against only one agent,<br><br>you'd <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=alternate between"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=alternate"><span style="border-bottom:1px solid #FF0000;">alternate</span></a></object> between</span></a></object> different version of the agent itself,<br><br>so it all, like, learns all this skill together.<br><br>It doesn't <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=overfit"><span style="border-bottom:1px solid #0000FF;">overfit</span></a></object> to this level.<br><br>So, there's another, uh,<br><br>thing called like, <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=meta"><span style="border-bottom:1px solid #0000FF;">meta</span></a></object> learning.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=meta"><span style="border-bottom:1px solid #0000FF;">Meta</span></a></object> learning is a whole different algorithm again,<br><br>[NOISE] and the, and the purpose is that<br><br>a lot of tasks are like similar to each other, right?<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=for example"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">For example</span></a></object>, walking to left and<br><br>walking to right are like walking in the front direction,<br><br>they're like same tasks, essentially.<br><br>[NOISE] So, the point is,<br><br>rather than training on a single task which is like go left or go right,<br><br>you train on a distribution of tasks that are similar to each other.<br><br>[NOISE] And then the idea is that,<br><br>for each specific task,<br><br>I should learn with like, uh,<br><br>very few gradient steps,<br><br>so very few updates should be enough for me.<br><br>So, if I learn,<br><br>okay, play this video, it's like, at the beginning,<br><br>this agent has been trained with <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=meta"><span style="border-bottom:1px solid #0000FF;">meta</span></a></object> learning before,<br><br>doesn't know how to move,<br><br>but just look at the number of gradient steps, like,<br><br>after two or three gradient steps,<br><br>it totally knows how to move.<br><br>That's, th - that's normally takes a lot of the steps to train,<br><br>[NOISE] but that's only because of the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=meta"><span style="border-bottom:1px solid #0000FF;">meta</span></a></object> learning approach that we've used here.<br><br>[NOISE] <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=meta"><span style="border-bottom:1px solid #0000FF;">Meta</span></a></object> learning is also cool, I mean, uh,<br><br>the algorithm is from <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=berkeley"><span style="border-bottom:1px solid #FF0000;">Berkeley</span></a></object>, Chelsea <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=finn"><span style="border-bottom:1px solid #FF0000;">Finn</span></a></object>,<br><br>which is now also coming to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=stanford"><span style="border-bottom:1px solid #FF0000;">Stanford</span></a></object>.<br><br>It's called Model-Agnostic Meta-Learning.<br><br>[NOISE] So, all right.<br><br>Another point this, uh,<br><br>very interesting game, <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=montezuma's revenge"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">Montezuma's Revenge</span></a></object>,<br><br>that <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=kian"><span style="border-bottom:1px solid #0000FF;">Kian</span></a></object> talked.<br><br>How much time do we have?<br><br>[inaudible].<br><br>All right. Uh, yeah. So, you've seen,<br><br>uh, exploration-exploitation dilemma, right?<br><br>So it's, it's, it's bad if you don't <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=ex"><span style="border-bottom:1px solid #FF0000;">ex</span></a></object> - explore,<br><br>you gonna fail many times.<br><br>So, if you do the exploration with the scheme<br><br>that you just saw like epsilon-greedy,<br><br>this is a map of the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=montezuma"><span style="border-bottom:1px solid #800000;">Montezuma</span></a></object> game,<br><br>and you gonna see all different moves of that game,<br><br>if you do like, exploration <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=randomly"><span style="border-bottom:1px solid #FF0000;">randomly</span></a></object>.<br><br>And, uh, the game, I think, has, like,<br><br>21 or 20 something different rooms that is hard to reach.<br><br>[NOISE] So, there's this recent paper I<br><br>think from <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=google"><span style="border-bottom:1px solid #0000FF;">Google</span></a></object> Brain from <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=marc"><span style="border-bottom:1px solid #FF0000;">Marc</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=bellemare"><span style="border-bottom:1px solid #0000FF;">Bellemare</span></a></object> and team.<br><br>It's called <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=unifying"><span style="border-bottom:1px solid #FF0000;">Unifying</span></a></object> the Count-based <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=metas"><span style="border-bottom:1px solid #0000FF;">Metas</span></a></object> for Exploration.<br><br>Exploration's essentially a very hard challenge,<br><br>mostly in the situation that the reward is a <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=sparse"><span style="border-bottom:1px solid #FF0000;">sparse</span></a></object>.<br><br>For exactly, in this game,<br><br>the first reward that you get is when you reach the key, right?<br><br>[NOISE] And from t - top to here,<br><br>it's almost like 200 a steps,<br><br>and g - getting the number of actions after 200 steps exactly right by,<br><br>like, random exploration, is almost impossible, so you're never gonna do that.<br><br>[NOISE] What, uh, a very interesting trick here is<br><br>that you're kind of keeping counts on how many times you visited a state,<br><br>[NOISE] and then if you visit a state,<br><br>that is, s - uh, [NOISE] that has like, uh,<br><br>fewer counts, then you give a reward to the agent,<br><br>so we call it the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=intrinsic reward"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=intrinsic"><span style="border-bottom:1px solid #FF0000;">intrinsic</span></a></object> reward</span></a></object>.<br><br>So, it kind of makes the -<br><br>Let's change your <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=mic"><span style="border-bottom:1px solid #800000;">mic</span></a></object> really quick. [NOISE]<br><br>[LAUGHTER] Right here, I keep it.<br><br>[NOISE] S - so, it changes the,<br><br>[NOISE] looking for the reward is [inaudible] environment is also,<br><br>intense you up, like it,<br><br>it has <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=the instant"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">the instant</span></a></object> reach because you go and search around<br><br>because you gotta increase the counts of the state that it has never seen before.<br><br>So, this gets the agent to actually explore and look more,<br><br>so it just [NOISE] goes down usually like different rooms and stuff like that.<br><br>[NOISE] So, these identities,<br><br>and this game is interesting, if you search this,<br><br>there's a lot of people that recently are trying to solve the game,<br><br>as it includes research on that Montezuma's is one of the game,<br><br>and it's just f - fun also to see the agent play.<br><br>Any question on that? [NOISE]<br><br>[inaudible]<br><br>[LAUGHTER] Any question? [NOISE] Well, I -<br><br>There is also [NOISE] another interesting [NOISE] point<br><br>that would be just fun to know about.<br><br>It's called <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=imitation"><span style="border-bottom:1px solid #FF0000;">imitation</span></a></object> learning.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=imitation"><span style="border-bottom:1px solid #FF0000;">Imitation</span></a></object> learning is the case that,<br><br>well, I mean, RL agents,<br><br>so sometimes you don't know the reward, like,<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=for example"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">for example</span></a></object>, the <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=atari"><span style="border-bottom:1px solid #800000;">Atari</span></a></object> games,<br><br>their reward is like, very well-defined, right?<br><br>If I get the key, I get the reward,<br><br>that's just obvious, but sometimes,<br><br>like, defining the reward is hard.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=for example"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">For example</span></a></object>, when the car, like the blue one,<br><br>wanna drive in a,<br><br>in some highway, what is the definition of the reward, right?<br><br>So, we don't have a clear definition of that.<br><br>But, on the other hand, you have a person,<br><br>like you have human expert that can drive for us,<br><br>and then this is, "Oh, this is the right way of driving," right?<br><br>So, in this situation,<br><br>we have something called <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=imitation"><span style="border-bottom:1px solid #FF0000;">imitation</span></a></object> learning that you try<br><br>to <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=mimic"><span style="border-bottom:1px solid #FF0000;">mimic</span></a></object> the behavior of a expert.<br><br>[NOISE] So, not exactly copying this,<br><br>because if we copy this and then you show us it completely different states,<br><br>then we don't know what to do,<br><br>but from now, we learn.<br><br>And this is like, <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=for example"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">for example</span></a></object>,<br><br>and there is a paper that called <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=generative"><span style="border-bottom:1px solid #800000;">Generative</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=adversarial"><span style="border-bottom:1px solid #FF0000;">Adversarial</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=imitation"><span style="border-bottom:1px solid #FF0000;">Imitation</span></a></object> Learning,<br><br>which was, like, from Stefano's group here at <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=stanford"><span style="border-bottom:1px solid #FF0000;">Stanford</span></a></object>,<br><br>and that was also interesting.<br><br>[NOISE] Well, I think that's advanced topic.<br><br>If you have any questions, I'm here.<br><br><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=kian"><span style="border-bottom:1px solid #0000FF;">Kian</span></a></object>. [NOISE] Question?<br><br>[NOISE] No? [NOISE]<br><br>Okay. Sorry. Just, uh, for,<br><br>for, uh, next week,<br><br>so there is no assignment.<br><br>We have not finished at C-5 and you know about sequence models now.<br><br>Uh, we all need to take a lot of time for this project.<br><br>The project's the big part of this because this has no, has, um,<br><br>[NOISE] there's gonna be, um,<br><br>[NOISE] project team <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=mentorship"><span style="border-bottom:1px solid #800000;">mentorship</span></a></object>.<br><br>And this Friday, we'll have these sections with reading research papers.<br><br>We go over the, the, <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=object detection"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;">object <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=detection"><span style="border-bottom:1px solid #FF0000;">detection</span></a></object></span></a></object> YOLO<br><br>and YOLO v2 papers from <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=redmon"><span style="border-bottom:1px solid #0000FF;">Redmon</span></a></object> <object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=et al"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=et al"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=et al"><span style="box-shadow: 0px 3px white, 0px 4px #00BFFF;"><object><a style="text-decoration:none;color:#000000" target="_blank" href="https://cn.bing.com/dict/search?q=et"><span style="border-bottom:1px solid #FF0000;">et</span></a></object> al</span></a></object></span></a></object></span></a></object>.<br><br>Okay. See you guys. Thank you.</p></p>
<!-- dict -->

<p><br></p>
<h4 id="_2">知识点</h4>
<p><b><font color="#FF0000">重点词汇</font></b><br><a target="_blank" href="https://cn.bing.com/dict/search?q=infinite">infinite</a> [ˈɪnfɪnət] n. 无限；[数] 无穷大；无限的东西（如空间，时间） adj. 无限的，无穷的；无数的；极大的 {cet4 cet6 ky ielts :6045}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=detection">detection</a> [dɪˈtekʃn] n. 侦查，探测；发觉，发现；察觉 {cet4 cet6 gre :6133}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=denoted">denoted</a> [diˈnəutid] 表示，指示（denote的过去式和过去分词） { :6148}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=tricky">tricky</a> [ˈtrɪki] adj. 狡猾的；机警的 { :6391}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=randomly">randomly</a> ['rændəmlɪ] adv. 随便地，任意地；无目的地，胡乱地；未加计划地 { :6507}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=francisco">francisco</a> [fræn'sɪskəʊ] n. 弗朗西斯科（男子名，等于Francis） { :6607}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=advancements">advancements</a> [ædˈvænsmənts] n. （级别的）晋升( advancement的名词复数 ); 前进; 进展; 促进 { :6629}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=inaudible">inaudible</a> [ɪnˈɔ:dəbl] adj. 听不见的；不可闻的 { :6808}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=algorithm">algorithm</a> [ˈælgərɪðəm] n. [计][数] 算法，运算法则 { :6819}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=algorithms">algorithms</a> [ˈælɡəriðəmz] n. [计][数] 算法；算法式（algorithm的复数） { :6819}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=mimic">mimic</a> [ˈmɪmɪk] vt. 模仿，摹拟 n. 效颦者，模仿者；仿制品；小丑 adj. 模仿的，模拟的；假装的 {toefl ielts gre :6833}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=beforehand">beforehand</a> [bɪˈfɔ:hænd] adj. 提前的；预先准备好的 adv. 事先；预先 {cet4 cet6 ky toefl ielts :6844}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=commuting">commuting</a> [kə'mju:tɪŋ] n. 乘公交车上下班；经常往来 { :6867}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=chess">chess</a> [tʃes] n. 国际象棋，西洋棋 n. (Chess)人名；(英)切斯 {zk gk cet4 cet6 ky :6948}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=unstable">unstable</a> [ʌnˈsteɪbl] adj. 不稳定的；动荡的；易变的 {cet4 cet6 toefl :6975}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=imitation">imitation</a> [ˌɪmɪˈteɪʃn] n. 模仿，仿造；仿制品 adj. 人造的，仿制的 {cet6 ky toefl ielts gre :6994}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=optimal">optimal</a> [ˈɒptɪməl] adj. 最佳的；最理想的 {cet6 toefl :7002}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=myriads">myriads</a> ['mɪrɪədz] n. 无数，极大数量( myriad的名词复数 ) { :7106}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=gradient">gradient</a> [ˈgreɪdiənt] n. [数][物] 梯度；坡度；倾斜度 adj. 倾斜的；步行的 {cet6 toefl :7370}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=gradients">gradients</a> [ˈgreɪdi:ənts] n. 渐变，[数][物] 梯度（gradient复数形式） { :7370}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=alternate">alternate</a> [ɔ:lˈtɜ:nət] n. 替换物 adj. 交替的；轮流的 vt. 使交替；使轮流 vi. 交替；轮流 {cet6 ky toefl ielts gre :7396}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=marc">marc</a> [mɑ:k] n. 机读目录；（水果，种子等经压榨后的）榨渣 n. (Marc)人名；(塞)马尔茨；(德、俄、法、荷、罗、瑞典、西、英)马克 { :7422}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=intrinsic">intrinsic</a> [ɪnˈtrɪnsɪk] adj. 本质的，固有的 {cet6 ky toefl ielts :7449}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=reinforcement">reinforcement</a> [ˌri:ɪnˈfɔ:smənt] n. 加固；增援；援军；加强 { :7506}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=wh">wh</a> [ ] abbr. 瓦特小时（Watt Hours）；白宫（White House）；白色（white） { :7515}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=idle">idle</a> [ˈaɪdl] adj. 闲置的；懒惰的；停顿的 vt. 虚度；使空转 vi. 无所事事；虚度；空转 {cet4 cet6 ky ielts gre :7526}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=phd">phd</a> [ ] abbr. 博士学位；哲学博士学位（Doctor of Philosophy） {ielts :7607}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=expectancy">expectancy</a> [ɪkˈspektənsi] n. 期望，期待 {ielts :7655}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=blog">blog</a> [blɒg] n. 博客；部落格；网络日志 { :7748}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=Et">Et</a> ['i:ti:] conj. （拉丁语）和（等于and） { :7820}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=compute">compute</a> [kəmˈpju:t] n. 计算；估计；推断 vt. 计算；估算；用计算机计算 vi. 计算；估算；推断 {cet4 cet6 ky toefl ielts :7824}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=dropout">dropout</a> [ˈdrɒpaʊt] n. 中途退学；辍学学生 {ielts :7969}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=unifying">unifying</a> [ˈju:nifaiŋ] 使统一；（unify的ing形式）使成一体 { :8008}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=Finn">Finn</a> [fin] n. 芬兰人 爱尔兰巨人 { :8047}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=converged">converged</a> [kən'vɜ:dʒd] v. 聚集，使会聚（converge的过去式） adj. 收敛的；聚合的 { :8179}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=converge">converge</a> [kənˈvɜ:dʒ] vt. 使汇聚 vi. 聚集；靠拢；收敛 {cet6 toefl ielts gre :8179}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=converges">converges</a> [kənˈvə:dʒz] v. （线条、运动的物体等）会于一点( converge的第三人称单数 ); （趋于）相似或相同; 人或车辆汇集; 聚集 { :8179}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=berkeley">berkeley</a> ['bɑ:kli, 'bә:kli] n. 贝克莱（爱尔兰主教及哲学家）；伯克利（姓氏）；伯克利（美国港市） { :8189}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=hack">hack</a> [hæk] n. 砍，劈；出租马车 vt. 砍；出租 vi. 砍 n. (Hack)人名；(英、西、芬、阿拉伯、毛里求)哈克；(法)阿克 {gre :8227}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=hacks">hacks</a> [hæks] n. (Hacks)人名；(德)哈克斯 老马（hack的复数） 出租汽车 { :8227}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=pi">pi</a> [paɪ] abbr. 产品改进（Product Improve） { :8364}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=intuitive">intuitive</a> [ɪnˈtju:ɪtɪv] adj. 直觉的；凭直觉获知的 {gre :8759}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=derivative">derivative</a> [dɪˈrɪvətɪv] n. [化学] 衍生物，派生物；导数 adj. 派生的；引出的 {toefl gre :9140}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=convergence">convergence</a> [kən'vɜ:dʒəns] n. [数] 收敛；会聚，集合 n. (Convergence)人名；(法)孔韦尔让斯 { :9173}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=proxy">proxy</a> [ˈprɒksi] n. 代理人；委托书；代用品 {toefl ielts :9178}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=paddle">paddle</a> [ˈpædl] n. 划桨；明轮翼 vt. 拌；搅；用桨划 vi. 划桨；戏水；涉水 {gk ky toefl ielts :9187}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=sub">sub</a> [sʌb] n. 潜水艇；地铁；替补队员 vi. 代替 { :9196}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=replay">replay</a> [ˈri:pleɪ] n. 重赛；重播；重演 vt. 重放；重演；重新比赛 { :9256}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=neural">neural</a> [ˈnjʊərəl] adj. 神经的；神经系统的；背的；神经中枢的 n. (Neural)人名；(捷)诺伊拉尔 { :9310}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=fran">fran</a> [fræn] abbr. framed-structure analysis 框架分析; franchise 特权，公民权 { :9383}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=sparse">sparse</a> [spɑ:s] adj. 稀疏的；稀少的 {toefl ielts gre :9557}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=invaders">invaders</a> [ɪn'veɪdəz] n. 侵略者（invader的复数）；侵入种 { :9804}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=approximate">approximate</a> [əˈprɒksɪmət] adj. [数] 近似的；大概的 vt. 近似；使…接近；粗略估计 vi. 接近于；近似于 {cet4 cet6 ky toefl ielts gre :9895}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=robotics">robotics</a> [rəʊˈbɒtɪks] n. 机器人学 { :10115}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=carlo">carlo</a> ['kɑrloʊ] n. 卡洛（男子名） { :10119}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=propagate">propagate</a> [ˈprɒpəgeɪt] vt. 传播；传送；繁殖；宣传 vi. 繁殖；增殖 {cet6 toefl ielts gre :10193}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=propagating">propagating</a> [ˈprɔpəɡeitɪŋ] v. 传播（propagate的ing形式）；繁殖 adj. 传播的；繁殖的 { :10193}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=Monte">Monte</a> ['mɒntɪ] n. 始于西班牙的纸牌赌博游戏 n. (Monte)人名；(英)蒙特(教名Montague的昵称)；(意、葡、瑞典)蒙特 { :10325}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=pixels">pixels</a> ['pɪksəl] n. [电子] 像素；像素点（pixel的复数） { :10356}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=pixel">pixel</a> [ˈpɪksl] n. （显示器或电视机图象的）像素（等于picture element） { :10356}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=generalize">generalize</a> [ˈdʒenrəlaɪz] vi. 形成概念 vt. 概括；推广；使...一般化 {cet6 ky toefl ielts gre :10707}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=gamma">gamma</a> [ˈgæmə] n. 微克；希腊语的第三个字母 n. (Gamma)人名；(法)加马；(阿拉伯)贾马 {toefl :10849}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=tweaked">tweaked</a> [twiːk] 拧（tweak的过去分词） 调整（tweak的过去分词） { :10855}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=tweak">tweak</a> [twi:k] n. 扭；拧；焦急 vt. 扭；用力拉；开足马力 { :10855}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=mute">mute</a> [mju:t] adj. 哑的；沉默的；无声的 vt. 减弱……的声音；使……柔和 n. 哑巴；弱音器；闭锁音 n. (Mute)人名；(塞)穆特 {cet4 cet6 ky gre :10937}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=infinity">infinity</a> [ɪnˈfɪnəti] n. 无穷；无限大；无限距 {cet6 gre :11224}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=optimize">optimize</a> [ˈɒptɪmaɪz] vt. 使最优化，使完善 vi. 优化；持乐观态度 {ky :11612}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=reliably">reliably</a> [rɪ'laɪəblɪ] adv. 可靠地；确实地 { :11630}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=restart">restart</a> [ˌri:ˈstɑ:t] n. 重新开始；返聘 vt. [计] 重新启动；重新开始 vi. [计] 重新启动；重新开始 { :11902}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=ex">ex</a> [eks] n. 前妻或前夫 prep. 不包括，除外 { :12200}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=diverge">diverge</a> [daɪˈvɜ:dʒ] vt. 使偏离；使分叉 vi. 分歧；偏离；分叉；离题 {cet6 toefl gre :12262}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=Demo">Demo</a> [ˈdeməʊ] n. 演示；样本唱片；示威；民主党员 n. (Demo)人名；(意、阿尔巴)德莫 { :12334}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=propagation">propagation</a> [ˌprɒpə'ɡeɪʃn] n. 传播；繁殖；增殖 {cet6 gre :12741}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=computations">computations</a> [kɒmpjʊ'teɪʃnz] n. 计算，估计( computation的名词复数 ) { :12745}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=computation">computation</a> [ˌkɒmpjuˈteɪʃn] n. 估计，计算 {toefl :12745}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=epoch">epoch</a> [ˈi:pɒk] n. [地质] 世；新纪元；新时代；时间上的一点 {cet6 ky toefl ielts gre :12794}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=epochs">epochs</a> [ ] 时代（epoch的复数形式） 时期（epoch的复数形式） { :12794}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=intuitively">intuitively</a> [ɪn'tju:ɪtɪvlɪ] adv. 直观地；直觉地 { :14665}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=multidisciplinary">multidisciplinary</a> [ˌmʌltidɪsəˈplɪnəri] adj. 有关各种学问的 { :14907}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=adversarial">adversarial</a> [ˌædvəˈseəriəl] adj. 对抗的；对手的，敌手的 { :15137}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=breakout">breakout</a> [ˈbreɪkaʊt] n. 爆发；突围；越狱；脱逃 { :15289}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=stanford">stanford</a> ['stænfәd] n. 斯坦福（姓氏，男子名）；斯坦福大学（美国一所大学） { :15904}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=mu">mu</a> [mju:] n. 希腊语的第12个字母；微米 n. (Mu)人名；(中)茉(广东话·威妥玛) { :16619}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=meth">meth</a> [meθ] n. 甲安菲他明（一种兴奋剂） n. (Meth)人名；(柬)梅 { :16881}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=optimization">optimization</a> [ˌɒptɪmaɪ'zeɪʃən] n. 最佳化，最优化 {gre :16923}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=iteration">iteration</a> [ˌɪtəˈreɪʃn] n. [数] 迭代；反复；重复 { :17595}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=chan">chan</a> [tʃæn] n. 通道（槽，沟） n. (Chan)人名；(法)尚；(缅)钱；(柬、老、泰)占 { :17670}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=dataset">dataset</a> ['deɪtəset] n. 资料组 { :18096}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=BOT">BOT</a> [bɒt] n. 马蝇幼虫，马蝇 n. (Bot)人名；(俄、荷、罗、匈)博特；(法)博 { :18864}<br><br><br><b><font color="#800000">难点词汇</font></b><br><a target="_blank" href="https://cn.bing.com/dict/search?q=exhaustively">exhaustively</a> [ɪɡ'zɔ:stɪvlɪ] adv. 耗尽一切地 { :20316}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=MIC">MIC</a> [maɪk] abbr. 军界，工业界集团（Military-Industrial Complex） n. (Mic)人名；(罗)米克 { :21352}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=generative">generative</a> [ˈdʒenərətɪv] adj. 生殖的；生产的；有生殖力的；有生产力的 { :21588}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=epsilon">epsilon</a> [ˈepsɪlɒn] n. 希腊语字母之第五字 { :22651}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=locomotion">locomotion</a> [ˌləʊkəˈməʊʃn] n. 运动；移动；旅行 {toefl gre :22712}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=dynamically">dynamically</a> [daɪ'næmɪklɪ] adv. 动态地；充满活力地；不断变化地 { :23174}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=recap">recap</a> [ˈri:kæp] n. 翻新的轮胎 vt. 翻新胎面；扼要重述 { :23344}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=chessboard">chessboard</a> [ˈtʃesbɔ:d] n. 棋盘 { :23620}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=pong">pong</a> [pɒŋ] n. 乒乓球；恶臭；难闻的气味 adj. 乒乓的 vi. 发出难闻的气味 n. (Pong)人名；(东南亚国家华语)榜；(柬)邦；(泰)蓬；(中)庞(广东话·威妥玛) { :23635}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=pseudo">pseudo</a> ['sju:dəʊ] n. 伪君子；假冒的人 adj. 冒充的，假的 { :24260}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=phi">phi</a> [faɪ] n. 希腊文的第21个字母 n. (Phi)人名；(柬、老)披 { :24548}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=iterative">iterative</a> ['ɪtərətɪv] adj. [数] 迭代的；重复的，反复的 n. 反复体 { :25217}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=xavier">xavier</a> ['zʌvɪə] n. 泽维尔（男子名） { :26299}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=bellman">bellman</a> ['belmən] n. 更夫；传达员；鸣钟者 { :26872}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=mentorship">mentorship</a> ['mentɔːʃɪp] n. 导师制，辅导教师；师徒制 { :27920}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=Boolean">Boolean</a> [ ] adj. 布尔数学体系的 { :27921}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=stochastic">stochastic</a> [stə'kæstɪk] adj. [数] 随机的；猜测的 { :28398}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=dw">dw</a> [ ] abbr. 发展的宽度（Developed Width）；蒸馏水（Distilled Water）；双重墙（Double Wall）；双重载（Double Weight） { :29507}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=Atari">Atari</a> [ ] n. 雅达利（美国一家电脑游戏机厂商） { :29876}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=dimensionality">dimensionality</a> [dɪˌmenʃə'nælɪtɪ] n. 维度；幅员；广延 { :29902}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=optimality">optimality</a> [ɒptɪ'mælɪtɪ] n. [数] 最佳性 { :30883}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=tuple">tuple</a> [tʌpl] n. 元组，数组 { :31456}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=luminance">luminance</a> [ˈlu:mɪnəns] n. [光][电子] 亮度；[光] 发光性（等于luminosity）；光明 { :32601}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=shortsighted">shortsighted</a> ['ʃɔ:t'saɪtɪd] adj. 目光短浅的；近视的 { :32694}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=raster">raster</a> ['ræstə] n. [电子] 光栅；试映图 { :33252}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=initialized">initialized</a> [ɪ'nɪʃlaɪzd] adj. 初始化；初始化的；起始步骤 v. 初始化（initialize的过去分词）；预置 { :37736}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=initializing">initializing</a> [ ] 正在初始化 { :37736}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=initialize">initialize</a> [ɪˈnɪʃəlaɪz] vt. 初始化 { :37736}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=classifier">classifier</a> [ˈklæsɪfaɪə(r)] n. [测][遥感] 分类器； { :37807}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=DL">DL</a> [ ] abbr. 分升（deciliter）；数据传输线路（Data Link）；基准面（Datam Ievel）；延迟线（Delay Line） { :39786}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=initialization">initialization</a> [ɪˌnɪʃəlaɪ'zeɪʃn] n. [计] 初始化；赋初值 { :40016}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=iteratively">iteratively</a> [ ] [计] 迭代的 { :48568}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=Montezuma">Montezuma</a> [ ] 蒙特苏马 { :49277}<br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=melo">melo</a> ['meləʊ] n. &lt;口&gt;情节剧 { :49586}<br><br><br><b><font color="#0000FF">生僻词</font></b><br><a target="_blank" href="https://cn.bing.com/dict/search?q=abled">abled</a> ['eibld] a. [前面往往带副词] (体格)强壮的, 身体(或体格)健全的；无残疾的 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=approximator">approximator</a> [ə'prɒksɪmeɪtə] n. 接近者, 近似者 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=approximators">approximators</a> [ ] [网络] 变动型模糊限制语 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=backpropagate">backpropagate</a> [ ] [网络] 反向传播 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=bellemare">bellemare</a> [ ] n. (Bellemare)人名；(法)贝勒马尔 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=Brunskill">Brunskill</a> [ ] n. 布伦斯基尔 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=convolutional">convolutional</a> [kɒnvə'lu:ʃənəl] adj. 卷积的；回旋的；脑回的 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=DeepMind">DeepMind</a> [ ] n. 深刻的见解 [网络] 心灵深处；初恋汽水；深层思想 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=google">google</a> [ ] 谷歌；谷歌搜索引擎 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=grayscale">grayscale</a> ['grei,skeil] 灰度；灰度图；灰度级；灰度模式 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=incentivize">incentivize</a> [ɪn'sentɪvaɪz] 以物质刺激鼓励 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=kian">kian</a> [ ] [网络] 奇恩；奇安；吉安 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=meta">meta</a> [ ] [计] 元的 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=metas">metas</a> ['metəz] abbr. metastasis 转移; metastasize 转移 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=overfit">overfit</a> [ ] [网络] 过拟合；过度拟合；过适应 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=preprocess">preprocess</a> [pri:'prəʊses] vt. 预处理；预加工 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=preprocessing">preprocessing</a> [prep'rəʊsesɪŋ] n. 预处理；预加工 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=pseudocode">pseudocode</a> ['sju:dəʊˌkəʊd] n. 伪代码；假码；虚拟程序代码 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=redmon">redmon</a> [ ] [网络] 雷德蒙市；洪爷 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=relu">relu</a> [ ] [网络] 关节轴承 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=seaquest">seaquest</a> [ ] [网络] 水美净；海探险号 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=youtube">youtube</a> ['ju:tju:b] n. 视频网站（可以让用户免费上传、观赏、分享视频短片的热门视频共享网站） <br><br><br><b><font color="#00BFFF">词组</font></b><br><a target="_blank" href="https://cn.bing.com/dict/search?q=a grid">a grid</a> [ ] [网络] 一格；栅格；网格 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=a hack">a hack</a> [ ] [网络] 网络攻击 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=a robot">a robot</a> [ ] [网络] 一个机器人；到机器人 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=a trash">a trash</a> [ ] None <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=alternate between">alternate between</a> [ ] [网络] 时而…时而；在两种状态中交替变化 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=an algorithm">an algorithm</a> [ ] [网络] 规则系统；运算程式 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=back prop">back prop</a> [ ] 后撑;后支柱 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=back propagation">back propagation</a> [ˈbækˌprɔpəˈgeɪʃən] [网络] 反向传播；误差反向传播；反向传播算法 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=Bellman equation">Bellman equation</a> [ ] [数] 贝尔曼方程 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=bias towards">bias towards</a> [ ] [网络] 对……有利的偏见 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=class of algorithm">class of algorithm</a> [ ] 演算法类别 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=correlate with">correlate with</a> [ ] [网络] 找出一一对应的关系；与…相关；使相互关联 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=descent algorithm">descent algorithm</a> [ ] 下降算法 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=descent method">descent method</a> [ ] un. 下降法 [网络] 下山法；降方法 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=dimensionality reduction">dimensionality reduction</a> [ ] un. 维数减缩 [网络] 降维；维归约；维度缩减 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=et al">et al</a> [ ] abbr. 以及其他人，等人 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=et al.">et al.</a> [ˌet ˈæl] adv. 以及其他人；表示还有别的名字省略不提 abbr. 等等（尤置于名称后，源自拉丁文 et alii/alia） [网络] 等人；某某等人；出处 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=et. al">et. al</a> [ ] adv. 以及其他人；用在一个名字后面 [网络] 等；等人；等等 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=feedback loop">feedback loop</a> [ ] un. 反馈环路；反馈回路；回授电路 [网络] 反馈循环；回馈回路；反馈电路 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=forward propagation">forward propagation</a> [ ] 正向传播 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=from scratch">from scratch</a> [frɔm skrætʃ] adj. 从零开始的；白手起家的 [网络] 从头开始；从头做起；从无到有 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=garbage collector">garbage collector</a> [ˈɡɑ:bidʒ kəˈlektə] n. 收垃圾的 [网络] 垃圾收集器；垃圾回收器；被垃圾回收器 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=gradient algorithm">gradient algorithm</a> [ ] [网络] 其中包括梯度运算；其中包括剃度运算 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=gradient descent">gradient descent</a> [ ] n. 梯度下降法 [网络] 梯度递减；梯度下降算法；梯度递减的学习法 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=gradient descent algorithm">gradient descent algorithm</a> [ ] [网络] 梯度下降算法；梯度陡降法；梯度衰减原理 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=hack to">hack to</a> [ ] vt.劈,砍 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=imitation learning">imitation learning</a> [ ] 模仿学习 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=in bold">in bold</a> [ ] [网络] 粗体地；黑体地；粗体的 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=in the loop">in the loop</a> [ ] [网络] 灵通人士；通灵人士；圈内 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=infinite life">infinite life</a> [ ] 无限寿命 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=intrinsic reward">intrinsic reward</a> [ ] [网络] 内在奖励；内在报酬；内在的奖励 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=intrinsic rewards">intrinsic rewards</a> [ ] 内在报酬 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=iterative algorithm">iterative algorithm</a> [ ] [网络] 迭代算法；叠代演算法；迭代法 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=learning algorithm">learning algorithm</a> [ ] [网络] 学习演算法；学习算法；学习机制 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=life expectancy">life expectancy</a> [laif ɪkˈspektənsi:] n. 预期寿命；预计存在（或持续）的期限 [网络] 平均寿命；平均余命；期望寿命 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=loop break">loop break</a> [ ] 末端钩环 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=loop in">loop in</a> [ ] loop sb in，把某人拉进圈子 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=loop through">loop through</a> [ ] un. 电路接通 [网络] 依次通过；环通输出接口；环通输入接口 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=Monte Carlo">Monte Carlo</a> [mɔnti 'kɑ:lәu] n. 【旅】蒙特卡罗 [网络] 蒙特卡洛；蒙地卡罗；蒙特卡罗法 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=Montezuma's revenge">Montezuma's revenge</a> [ ] [网络] 魔宫寻宝；复仇；腹泻 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=multiply by">multiply by</a> [ ] v. 乘 [网络] 乘以；乘上；使相乘 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=neural network">neural network</a> [ˈnjuərəl ˈnetwə:k] n. 神经网络 [网络] 类神经网路；类神经网络；神经元网络 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=neural networks">neural networks</a> [ ] na. 【计】模拟脑神经元网络 [网络] 神经网络；类神经网路；神经网络系统 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=no derivative">no derivative</a> [ ] 禁止改作 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=object detection">object detection</a> [ ] [科技] 物体检测 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=optimize for">optimize for</a> [ ] vt.为...而尽可能完善 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=pixel image">pixel image</a> [ˈpiksəl ˈimidʒ]  [医]像素显像 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=play chess">play chess</a> [ ] na. 下象棋 [网络] 下棋；下国际象棋；着棋 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=plug in">plug in</a> [plʌɡ in] v. 插入；插插头；插上 [网络] 插件；连接；插上电源 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=plus infinity">plus infinity</a> [ ] [网络] 正无穷大；正无限大 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=pseudo code">pseudo code</a> [ˈsu:dəʊ kəud] n. 【计】伪码 [网络] 虚拟码；伪代码；假码 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=raster algorithm">raster algorithm</a> [ ] [计] 光栅算法 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=recycle bin">recycle bin</a> [ˌri:ˈsaikl bin] un. 回收站 [网络] 资源回收筒；资源回收桶；垃圾箱 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=reinforcement learning">reinforcement learning</a> [ ] 强化学习 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=reinforcement learning algorithm">reinforcement learning algorithm</a> [ ] [计] 强化式学习算法 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=space invader">space invader</a> [ ] 空间侵犯者：聊天时站得太靠近而侵犯到对方的个人空间，或坐在某人旁边时坐得太近而碰触到别人的脚或手臂的人。 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=stay idle">stay idle</a> [stei ˈaidl] [网络] 吃闲饭；投闲置散；显得没事干 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=terminal state">terminal state</a> [ ] 终点状态 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=the algorithm">the algorithm</a> [ ] [网络] 算法 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=the instant">the instant</a> [ðə ˈinstənt] [网络] 刹那；瞬间；我认许刹那 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=the Loop">the Loop</a> [ ] [网络] 主循环；大回圈区；卢普区 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=the matrix">the matrix</a> [ ] [网络] 黑客帝国；骇客任务；骇客帝国 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=the Max">the Max</a> [ ] [网络] 麦克斯；牛魔王；电子产品配件 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=the terminal">the terminal</a> [ ] [网络] 幸福终点站；航站情缘；机场客运站 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=to compute">to compute</a> [ ] [网络] 计算；用计算机计算 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=to initialize">to initialize</a> [ ] 初始化 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=training loop">training loop</a> [ ] 培训回路 <br><br><a target="_blank" href="https://cn.bing.com/dict/search?q=trash can">trash can</a> [træʃ kæn] n. （街道上或公共建筑物里的）垃圾箱 [网络] 垃圾桶；垃圾筒；垃圾箱图片 <br><br><br><b><font color="#00BFFF">惯用语</font></b><br><a target="_blank" href="https://cn.bing.com/dict/search?q=conv relu">conv relu</a><br><a target="_blank" href="https://cn.bing.com/dict/search?q=for example">for example</a><br><a target="_blank" href="https://cn.bing.com/dict/search?q=good point">good point</a><br><a target="_blank" href="https://cn.bing.com/dict/search?q=so now">so now</a><br><a target="_blank" href="https://cn.bing.com/dict/search?q=the reward change is dynamic">the reward change is dynamic</a><br><br><br><br><font size="2"><em> 单词释义末尾数字为词频顺序<br></em> zk/中考 gk/中考 ky/考研 cet4/四级 cet6/六级 ielts/雅思 toefl/托福 gre/GRE<br>* 词汇量测试建议用 testyourvocab.com<br></font></p>
<!-- end -->

<p><br><br></p>
<!-- comment -->

<div id="wpac-comment"></div>

<script type="text/javascript">

path=window.location.pathname.replace(/\/$/,'')

parts=path.split('/')
length=parts.length
if (length>=2)
{
    channel=parts[length-2]+'/'+parts[length-1]
}
else
{
    channel=path
}


wpac_init = window.wpac_init || [];
wpac_init.push({widget: 'Comment', id: 22426, chan:channel});
(function() {
    if ('WIDGETPACK_LOADED' in window) return;
    WIDGETPACK_LOADED = true;
    var mc = document.createElement('script');
    mc.type = 'text/javascript';
    mc.async = true;
    mc.src = 'https://embed.widgetpack.com/widget.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
})();
</script>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            <a href="https://creativecommons.org/licenses/by-nc/4.0/deed.zh"><font color="white">CC BY-NC</font></a>
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../../assets/javascripts/application.2ba8dec4.js"></script>
      
      <script>app.initialize({version:"1.1",url:{base:"../../../.."}})</script>
      
    
  </body>
</html>